{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths to datasets\n",
    "train_path = r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\train.csv\"\n",
    "test_path = r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\test.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def import_data(path, index_col=None):\n",
    "    \"\"\"Import data from a CSV file and optimize memory usage.\"\"\"\n",
    "    df = pd.read_csv(path, index_col=index_col)\n",
    "    return reduce_mem_usage(df)\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\"Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\"\"\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if isinstance(col_type, pd.IntervalDtype):\n",
    "            continue\n",
    "\n",
    "        if str(col_type)[:3] == 'int':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                df[col] = df[col].astype(np.int64)  \n",
    "        elif str(col_type)[:5] == 'float':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float64)\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"Feature engineering on the dataset.\"\"\"\n",
    "    # Binning age and converting to categorical labels instead of intervals\n",
    "    age_bins = pd.cut(df['Age'], bins=7, labels=False)\n",
    "    df['Age_Type'] = age_bins\n",
    "    df['Vehicle_Age'] = df['Vehicle_Age'].astype('category').cat.codes\n",
    "    df['Vehicle_Damage'] = df['Vehicle_Damage'].astype('category').cat.codes\n",
    "    df['Previously_Insured'] = df['Previously_Insured'].astype('category').cat.codes\n",
    "\n",
    "    df['Age_x_Vehicle_Age'] = df['Age_Type'] * df['Vehicle_Age']\n",
    "    df['Age_x_Vehicle_Damage'] = df['Age_Type'] * df['Vehicle_Damage']\n",
    "    df['Age_x_Previously_Insured'] = df['Age_Type'] * df['Previously_Insured']\n",
    "\n",
    "    fac_pre = ['Policy_Sales_Channel', 'Vehicle_Damage', 'Annual_Premium', 'Vintage', 'Age_Type']\n",
    "    col_pre = []\n",
    "    for i in fac_pre:\n",
    "        df['Previously_Insured_x_' + i] = pd.factorize(df['Previously_Insured'].astype(str) + df[i].astype(str))[0]\n",
    "        col_pre.append('Previously_Insured_x_' + i)\n",
    "\n",
    "    fac_pro = fac_pre[1:]\n",
    "    col_pro = []\n",
    "    for i in fac_pro:\n",
    "        df['Policy_Sales_Channel_x_' + i] = pd.factorize(df['Policy_Sales_Channel'].astype(str) + df[i].astype(str))[0]\n",
    "        col_pro.append('Policy_Sales_Channel_x_' + i)\n",
    "    return df, col_pre, col_pro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and optimize data\n",
    "train_df = import_data(train_path, index_col='id')\n",
    "test_df = import_data(test_path, index_col='id')\n",
    "\n",
    "# Combine train and test datasets for consistent transformation\n",
    "full_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# Convert columns to category type\n",
    "less = ['Gender', 'Vehicle_Age', 'Vehicle_Damage', 'Policy_Sales_Channel']\n",
    "for col in less:\n",
    "    full_df[col] = full_df[col].astype('category')\n",
    "\n",
    "# Apply feature engineering to the combined dataset\n",
    "full_df, col_pre, col_pro = feature_engineering(full_df)\n",
    "\n",
    "# Split back into train and test sets\n",
    "train_df = full_df.iloc[:len(train_df), :]\n",
    "test_df = full_df.iloc[len(train_df):, :]\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X = train_df.drop('Response', axis=1)\n",
    "y = train_df['Response']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4967"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the ColumnTransformer\n",
    "coltrans = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(sparse_output=False, dtype=np.float32), ['Gender', 'Vehicle_Damage']),\n",
    "        ('minmax', MinMaxScaler(), ['Age', 'Region_Code', 'Previously_Insured', 'Policy_Sales_Channel', 'Vintage']),\n",
    "        ('ordinal', OrdinalEncoder(categories=[[0, 1, 2]], dtype=np.float32), ['Vehicle_Age']),\n",
    "        ('robust', RobustScaler(), ['Annual_Premium']),\n",
    "        ('standard', StandardScaler(), ['Age_Type', 'Age_x_Vehicle_Age', 'Age_x_Vehicle_Damage', 'Age_x_Previously_Insured']),\n",
    "        ('standard_2', StandardScaler(), col_pre + col_pro),\n",
    "    ],\n",
    "    remainder='passthrough'  # Keeps columns not specified in transformers\n",
    ")\n",
    "\n",
    "# Fit the transformer on the training data and transform both training and validation sets\n",
    "X_train = coltrans.fit_transform(X_train)\n",
    "X_valid = coltrans.transform(X_valid)\n",
    "test_df = coltrans.transform(test_df.drop('Response', axis=1))\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "[LightGBM] [Warning] Unknown parameter: depth\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: depth\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.301075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91619\n",
      "[LightGBM] [Info] Number of data points in the train set: 7363070, number of used features: 25\n",
      "[LightGBM] [Warning] Unknown parameter: depth\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Info] Start training from score 0.122997\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the ratio for scale_pos_weight\n",
    "ratio = len(train_df[train_df['Response'] == 0]) / len(train_df[train_df['Response'] == 1])\n",
    "\n",
    "# Define LightGBM parameters\n",
    "lgb_params = {\n",
    "               'depth': 6,\n",
    "               \"eval_metric\": \"auc\",\n",
    "               \"early_stopping_round\": 50,\n",
    "               \"max_bin\": 262143,\n",
    "               'num_leaves': 223,\n",
    "               'learning_rate': 0.028095688623590447,\n",
    "               'min_child_samples': 54,\n",
    "               'subsample': 0.5395472919165504,\n",
    "               'colsample_bytree': 0.547518064129546,\n",
    "               'lambda_l1': 3.4444245446562,\n",
    "            #   'device': 'gpu',\n",
    "               'lambda_l2': 2.87490408088595e-05,\n",
    "               'scale_pos_weight': ratio,\n",
    "               'early_stopping_rounds': 10,}\n",
    "\n",
    "# Initialize lists to store out-of-fold predictions and AUC scores\n",
    "lgb_preds = []\n",
    "lgb_aucs = []\n",
    "\n",
    "# Train LightGBM model with cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(f\"Training fold {fold + 1}\")\n",
    "    X_train_fold, y_train_fold = X_train[train_idx], y_train.iloc[train_idx]\n",
    "    X_valid_fold, y_valid_fold = X_train[valid_idx], y_train.iloc[valid_idx]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    valid_data = lgb.Dataset(X_valid_fold, label=y_valid_fold)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[train_data, valid_data],\n",
    "\n",
    "    )\n",
    "    \n",
    "    valid_preds = model.predict(X_valid_fold, num_iteration=model.best_iteration)\n",
    "    auc_score = roc_auc_score(y_valid_fold, valid_preds)\n",
    "    lgb_aucs.append(auc_score)\n",
    "    \n",
    "    test_pred = model.predict(test_df, num_iteration=model.best_iteration)\n",
    "    lgb_preds.append(test_pred)\n",
    "    \n",
    "    # Save the model for this fold\n",
    "    model.save_model(f'lgb_model_fold_{fold + 1}.txt')\n",
    "    \n",
    "    # Clear memory\n",
    "    del X_train_fold, y_train_fold, X_valid_fold, y_valid_fold, train_data, valid_data, model\n",
    "    gc.collect()\n",
    "\n",
    "# Calculate overall AUC score for LightGBM\n",
    "auc_mean_lgb = np.mean(lgb_aucs)\n",
    "auc_std_lgb = np.std(lgb_aucs)\n",
    "# Average the predictions from each fold for LightGBM\n",
    "test_pred_lgb = np.mean(lgb_preds, axis=0)\n",
    "joblib.dump(test_pred_lgb, 'test_pred_lgb.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reimport the test_df to get the original index\n",
    "test_df = import_data(test_path, index_col='id')\n",
    "\n",
    "# Create a submission DataFrame using the original test index\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df.index,\n",
    "    'Response': test_pred_lgb\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission.to_csv('submission_lgb.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
