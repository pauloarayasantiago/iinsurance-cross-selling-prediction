{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import gc\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler, RobustScaler\n","from sklearn.compose import ColumnTransformer\n","from sklearn.metrics import roc_auc_score\n","import xgboost as xgb\n","import joblib\n","\n","# Paths to datasets\n","train_path = r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\train.csv\"\n","test_path = r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\test.csv\"\n","\n","def import_data(path, index_col=None):\n","    \"\"\"Import data from a CSV file and optimize memory usage.\"\"\"\n","    df = pd.read_csv(path, index_col=index_col)\n","    return reduce_mem_usage(df)\n","\n","def reduce_mem_usage(df):\n","    \"\"\"Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\"\"\n","    for col in df.columns:\n","        col_type = df[col].dtype\n","        if isinstance(col_type, pd.IntervalDtype):\n","            continue\n","\n","        if str(col_type)[:3] == 'int':\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                df[col] = df[col].astype(np.int8)\n","            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                df[col] = df[col].astype(np.int16)\n","            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                df[col] = df[col].astype(np.int32)\n","            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                df[col] = df[col].astype(np.int64)  \n","        elif str(col_type)[:5] == 'float':\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                df[col] = df[col].astype(np.float32)\n","            else:\n","                df[col] = df[col].astype(np.float64)\n","    return df\n","\n","def feature_engineering(df):\n","    \"\"\"Feature engineering on the dataset.\"\"\"\n","    # Binning age and converting to categorical labels instead of intervals\n","    # age_bins = pd.cut(df['Age'], bins=7, labels=False)\n","    # df['Age_Type'] = age_bins\n","    # df['Vehicle_Age'] = df['Vehicle_Age'].astype('category').cat.codes\n","    # df['Vehicle_Damage'] = df['Vehicle_Damage'].astype('category').cat.codes\n","    # df['Previously_Insured'] = df['Previously_Insured'].astype('category').cat.codes\n","\n","    # df['Age_x_Vehicle_Age'] = df['Age_Type'] * df['Vehicle_Age']\n","    # df['Age_x_Vehicle_Damage'] = df['Age_Type'] * df['Vehicle_Damage']\n","    # df['Age_x_Previously_Insured'] = df['Age_Type'] * df['Previously_Insured']\n","\n","    fac_pre = ['Policy_Sales_Channel', 'Vehicle_Damage', 'Annual_Premium', 'Vintage', \n","            #  'Age_Type'\n","            ]\n","    col_pre = []\n","    for i in fac_pre:\n","        df['Previously_Insured_x_' + i] = pd.factorize(df['Previously_Insured'].astype(str) + df[i].astype(str))[0]\n","        col_pre.append('Previously_Insured_x_' + i)\n","\n","    fac_pro = fac_pre[1:]\n","    col_pro = []\n","    for i in fac_pro:\n","        df['Policy_Sales_Channel_x_' + i] = pd.factorize(df['Policy_Sales_Channel'].astype(str) + df[i].astype(str))[0]\n","        col_pro.append('Policy_Sales_Channel_x_' + i)\n","    return df, col_pre, col_pro\n","\n","def reduce_mem_usage_np(array):\n","    \"\"\"Reduce memory usage of a numpy array by converting its dtype without losing significant precision.\"\"\"\n","    for col in range(array.shape[1]):\n","        col_data = array[:, col]\n","        if np.issubdtype(col_data.dtype, np.floating):\n","            col_min = col_data.min()\n","            col_max = col_data.max()\n","            if col_min > np.finfo(np.float16).min and col_max < np.finfo(np.float16).max:\n","                array[:, col] = col_data.astype(np.float16)\n","            elif col_min > np.finfo(np.float32).min and col_max < np.finfo(np.float32).max:\n","                array[:, col] = col_data.astype(np.float32)\n","        elif np.issubdtype(col_data.dtype, np.integer):\n","            col_min = col_data.min()\n","            col_max = col_data.max()\n","            if col_min > np.iinfo(np.int8).min and col_max < np.iinfo(np.int8).max:\n","                array[:, col] = col_data.astype(np.int8)\n","            elif col_min > np.iinfo(np.int16).min and col_max < np.iinfo(np.int16).max:\n","                array[:, col] = col_data.astype(np.int16)\n","            elif col_min > np.iinfo(np.int32).min and col_max < np.iinfo(np.int32).max:\n","                array[:, col] = col_data.astype(np.int32)\n","    return array"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Load and optimize data\n","train_df = import_data(train_path, index_col='id')\n","test_df = import_data(test_path, index_col='id')\n","\n","# Combine train and test datasets for consistent transformation\n","full_df = pd.concat([train_df, test_df], axis=0)\n","\n","# Convert columns to category type\n","less = ['Gender', 'Vehicle_Age', 'Vehicle_Damage', 'Policy_Sales_Channel']\n","for col in less:\n","    full_df[col] = full_df[col].astype('category')\n","\n","# Apply feature engineering to the combined dataset\n","full_df, col_pre, col_pro = feature_engineering(full_df)\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Split back into train and test sets\n","train_df = full_df.iloc[:len(train_df), :]\n","test_df = full_df.iloc[len(train_df):, :]\n","\n","# Split the training data into training and validation sets\n","X = train_df.drop('Response', axis=1)\n","y = train_df['Response']\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"A given column is not a column of the dataframe","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\paulo\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n","\u001b[1;31mKeyError\u001b[0m: 'Age_Type'","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\paulo\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\sklearn\\utils\\__init__.py:505\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[1;32m--> 505\u001b[0m     col_idx \u001b[38;5;241m=\u001b[39m \u001b[43mall_columns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n","File \u001b[1;32mc:\\Users\\paulo\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n","\u001b[1;31mKeyError\u001b[0m: 'Age_Type'","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m      2\u001b[0m coltrans \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[0;32m      3\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      4\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m, OneHotEncoder(sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVehicle_Damage\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Keeps columns not specified in transformers\u001b[39;00m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Fit the transformer on the training data and transform both training and validation sets\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mcoltrans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m X_valid \u001b[38;5;241m=\u001b[39m coltrans\u001b[38;5;241m.\u001b[39mtransform(X_valid)\n\u001b[0;32m     17\u001b[0m test_df \u001b[38;5;241m=\u001b[39m coltrans\u001b[38;5;241m.\u001b[39mtransform(test_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n","File \u001b[1;32mc:\\Users\\paulo\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n","File \u001b[1;32mc:\\Users\\paulo\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\paulo\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:906\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[0;32m    904\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[1;32m--> 906\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n","File \u001b[1;32mc:\\Users\\paulo\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:496\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    494\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[0;32m    495\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[1;32m--> 496\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n","File \u001b[1;32mc:\\Users\\paulo\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\sklearn\\utils\\__init__.py:513\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    510\u001b[0m         column_indices\u001b[38;5;241m.\u001b[39mappend(col_idx)\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA given column is not a column of the dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n","\u001b[1;31mValueError\u001b[0m: A given column is not a column of the dataframe"]}],"source":["# Define the ColumnTransformer\n","coltrans = ColumnTransformer(\n","    transformers=[\n","        ('cat', OneHotEncoder(sparse_output=False, dtype=np.float32), ['Gender', 'Vehicle_Damage']),\n","        ('minmax', MinMaxScaler(), ['Age', 'Region_Code', 'Policy_Sales_Channel', 'Vintage']),\n","        ('ordinal', OrdinalEncoder(categories=[[0, 1, 2]], dtype=np.float32), ['Vehicle_Age']),\n","        ('robust', RobustScaler(), ['Annual_Premium']),\n","        ('standard', StandardScaler(), ['Age_x_Vehicle_Age', 'Age_x_Vehicle_Damage', 'Age_x_Previously_Insured']),\n","        ('standard_2', StandardScaler(), col_pre + col_pro),\n","    ],\n","    remainder='passthrough'  # Keeps columns not specified in transformers\n",")\n","\n","# Fit the transformer on the training data and transform both training and validation sets\n","X_train = coltrans.fit_transform(X_train)\n","X_valid = coltrans.transform(X_valid)\n","test_df = coltrans.transform(test_df.drop('Response', axis=1))\n","\n","# Reduce memory usage of transformed data\n","X_train = reduce_mem_usage_np(X_train)\n","X_valid = reduce_mem_usage_np(X_valid)\n","test_df = reduce_mem_usage_np(test_df)\n","\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training fold 1\n","[0]\ttrain-auc:0.85635\tvalid-auc:0.85656\n","[100]\ttrain-auc:0.87826\tvalid-auc:0.87745\n","[200]\ttrain-auc:0.88350\tvalid-auc:0.88135\n","[300]\ttrain-auc:0.88622\tvalid-auc:0.88303\n","[400]\ttrain-auc:0.88867\tvalid-auc:0.88445\n","[500]\ttrain-auc:0.89061\tvalid-auc:0.88545\n","[600]\ttrain-auc:0.89214\tvalid-auc:0.88611\n","[700]\ttrain-auc:0.89354\tvalid-auc:0.88666\n","[800]\ttrain-auc:0.89483\tvalid-auc:0.88715\n","[900]\ttrain-auc:0.89598\tvalid-auc:0.88758\n","[967]\ttrain-auc:0.89656\tvalid-auc:0.88771\n","Training fold 2\n","[0]\ttrain-auc:0.85634\tvalid-auc:0.85644\n","[100]\ttrain-auc:0.87831\tvalid-auc:0.87750\n","[200]\ttrain-auc:0.88327\tvalid-auc:0.88124\n","[300]\ttrain-auc:0.88624\tvalid-auc:0.88322\n","[400]\ttrain-auc:0.88879\tvalid-auc:0.88472\n","[500]\ttrain-auc:0.89054\tvalid-auc:0.88559\n","[600]\ttrain-auc:0.89239\tvalid-auc:0.88652\n","[700]\ttrain-auc:0.89388\tvalid-auc:0.88712\n","[800]\ttrain-auc:0.89510\tvalid-auc:0.88755\n","[900]\ttrain-auc:0.89617\tvalid-auc:0.88790\n","[1000]\ttrain-auc:0.89713\tvalid-auc:0.88820\n","[1100]\ttrain-auc:0.89807\tvalid-auc:0.88848\n","[1149]\ttrain-auc:0.89847\tvalid-auc:0.88855\n","Training fold 3\n","[0]\ttrain-auc:0.85641\tvalid-auc:0.85643\n","[100]\ttrain-auc:0.87828\tvalid-auc:0.87731\n","[200]\ttrain-auc:0.88318\tvalid-auc:0.88091\n","[300]\ttrain-auc:0.88610\tvalid-auc:0.88277\n","[400]\ttrain-auc:0.88844\tvalid-auc:0.88409\n","[500]\ttrain-auc:0.89036\tvalid-auc:0.88509\n","[600]\ttrain-auc:0.89205\tvalid-auc:0.88590\n","[700]\ttrain-auc:0.89348\tvalid-auc:0.88651\n","[800]\ttrain-auc:0.89492\tvalid-auc:0.88710\n","[900]\ttrain-auc:0.89606\tvalid-auc:0.88753\n","[1000]\ttrain-auc:0.89713\tvalid-auc:0.88788\n","[1100]\ttrain-auc:0.89802\tvalid-auc:0.88814\n","[1200]\ttrain-auc:0.89889\tvalid-auc:0.88836\n","[1300]\ttrain-auc:0.89988\tvalid-auc:0.88870\n","[1334]\ttrain-auc:0.90006\tvalid-auc:0.88871\n","Training fold 4\n","[0]\ttrain-auc:0.85656\tvalid-auc:0.85567\n","[100]\ttrain-auc:0.87832\tvalid-auc:0.87644\n","[200]\ttrain-auc:0.88333\tvalid-auc:0.88033\n","[300]\ttrain-auc:0.88664\tvalid-auc:0.88259\n","[400]\ttrain-auc:0.88900\tvalid-auc:0.88397\n","[500]\ttrain-auc:0.89079\tvalid-auc:0.88487\n","[600]\ttrain-auc:0.89245\tvalid-auc:0.88569\n","[700]\ttrain-auc:0.89370\tvalid-auc:0.88621\n","[800]\ttrain-auc:0.89488\tvalid-auc:0.88661\n","[893]\ttrain-auc:0.89591\tvalid-auc:0.88700\n","Training fold 5\n","[0]\ttrain-auc:0.85649\tvalid-auc:0.85611\n","[100]\ttrain-auc:0.87814\tvalid-auc:0.87697\n","[200]\ttrain-auc:0.88312\tvalid-auc:0.88076\n","[300]\ttrain-auc:0.88602\tvalid-auc:0.88268\n","[400]\ttrain-auc:0.88881\tvalid-auc:0.88446\n","[500]\ttrain-auc:0.89073\tvalid-auc:0.88546\n","[600]\ttrain-auc:0.89237\tvalid-auc:0.88626\n","[700]\ttrain-auc:0.89390\tvalid-auc:0.88699\n","[799]\ttrain-auc:0.89492\tvalid-auc:0.88730\n"]}],"source":["# Calculate the ratio for scale_pos_weight\n","ratio = len(train_df[train_df['Response'] == 0]) / len(train_df[train_df['Response'] == 1])\n","\n","# Define XGBoost parameters\n","xgb_params = {\n","    'random_state': 512,\n","    'objective': \"binary:logistic\",\n","    'eval_metric': 'auc',\n","    'max_depth': 8,\n","    'min_child_weight': 12,\n","    'colsample_bytree': 0.5,\n","    'gamma': 0.2,\n","    'learning_rate': 0.09093568107192034,\n","    'subsample': 1.0,\n","    'reg_alpha': 0.0011852827097616767,\n","    'reg_lambda': 1.0735757602378362e-06,\n","    'max_bin': 197818,\n","    'scale_pos_weight': ratio,  # Adjust this based on your dataset\n","    'tree_method': 'hist',  # Ensure your environment supports GPU\n","    'device': 'cuda',  # Ensure your environment supports GPU\n","}\n","\n","# Initialize lists to store out-of-fold predictions and AUC scores\n","xgb_preds = []\n","xgb_aucs = []\n","\n","# Train XGBoost model with cross-validation\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","for fold, (train_idx, valid_idx) in enumerate(skf.split(X_train, y_train)):\n","    print(f\"Training fold {fold + 1}\")\n","    X_train_fold, y_train_fold = X_train[train_idx], y_train.iloc[train_idx]\n","    X_valid_fold, y_valid_fold = X_train[valid_idx], y_train.iloc[valid_idx]\n","    \n","    dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold)\n","    dvalid = xgb.DMatrix(X_valid_fold, label=y_valid_fold)\n","    \n","    model = xgb.train(\n","        xgb_params,\n","        dtrain,\n","        num_boost_round=3000,\n","        evals=[(dtrain, 'train'), (dvalid, 'valid')],\n","        verbose_eval=100,\n","        early_stopping_rounds=10,\n","    )\n","    \n","    valid_preds = model.predict(dvalid, iteration_range=(0, model.best_iteration))\n","    auc_score = roc_auc_score(y_valid_fold, valid_preds)\n","    xgb_aucs.append(auc_score)\n","    \n","    dtest = xgb.DMatrix(test_df)\n","    test_pred = model.predict(dtest, iteration_range=(0, model.best_iteration))\n","    xgb_preds.append(test_pred)\n","    \n","    # Save the model for this fold\n","    model.save_model(f'xgb_model_fold_{fold + 1}.json')\n","    \n","    # Clear memory\n","    del X_train_fold, y_train_fold, X_valid_fold, y_valid_fold, dtrain, dvalid, model\n","    gc.collect()\n","\n","# Calculate overall AUC score for XGBoost\n","auc_mean_xgb = np.mean(xgb_aucs)\n","auc_std_xgb = np.std(xgb_aucs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["['test_pred_xgb.pkl']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Average the predictions from each fold for XGBoost\n","test_pred_xgb = np.mean(xgb_preds, axis=0)\n","joblib.dump(test_pred_xgb, 'test_pred_xgb.pkl')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\paulo\\AppData\\Local\\Temp\\ipykernel_10884\\3486884307.py:26: DeprecationWarning: is_interval_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.IntervalDtype)` instead\n","  if pd.api.types.is_interval_dtype(col_type):\n","C:\\Users\\paulo\\AppData\\Local\\Temp\\ipykernel_10884\\3486884307.py:26: DeprecationWarning: is_interval_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.IntervalDtype)` instead\n","  if pd.api.types.is_interval_dtype(col_type):\n","C:\\Users\\paulo\\AppData\\Local\\Temp\\ipykernel_10884\\3486884307.py:26: DeprecationWarning: is_interval_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.IntervalDtype)` instead\n","  if pd.api.types.is_interval_dtype(col_type):\n","C:\\Users\\paulo\\AppData\\Local\\Temp\\ipykernel_10884\\3486884307.py:26: DeprecationWarning: is_interval_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.IntervalDtype)` instead\n","  if pd.api.types.is_interval_dtype(col_type):\n","C:\\Users\\paulo\\AppData\\Local\\Temp\\ipykernel_10884\\3486884307.py:26: DeprecationWarning: is_interval_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.IntervalDtype)` instead\n","  if pd.api.types.is_interval_dtype(col_type):\n","C:\\Users\\paulo\\AppData\\Local\\Temp\\ipykernel_10884\\3486884307.py:26: DeprecationWarning: is_interval_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.IntervalDtype)` instead\n","  if pd.api.types.is_interval_dtype(col_type):\n","C:\\Users\\paulo\\AppData\\Local\\Temp\\ipykernel_10884\\3486884307.py:26: DeprecationWarning: is_interval_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.IntervalDtype)` instead\n","  if pd.api.types.is_interval_dtype(col_type):\n"]}],"source":["# Reimport the test_df to get the original index\n","test_df = import_data(test_path, index_col='id')\n","\n","# Create a submission DataFrame using the original test index\n","submission = pd.DataFrame({\n","    'id': test_df.index,\n","    'Response': test_pred_xgb\n","})\n","\n","# Save the submission DataFrame to a CSV file\n","submission.to_csv('submission_xgb.csv', index=False)\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8930475,"sourceId":73291,"sourceType":"competition"},{"sourceId":189806873,"sourceType":"kernelVersion"},{"sourceId":189806878,"sourceType":"kernelVersion"},{"sourceId":189813334,"sourceType":"kernelVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
