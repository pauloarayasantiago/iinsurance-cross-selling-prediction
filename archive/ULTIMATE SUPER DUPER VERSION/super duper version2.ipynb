{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import gc\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler, RobustScaler\n","from sklearn.compose import ColumnTransformer\n","from sklearn.metrics import roc_auc_score\n","import xgboost as xgb\n","import joblib\n","\n","# Paths to datasets\n","train_path = r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\train.csv\"\n","test_path = r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\test.csv\"\n","\n","def import_data(path, index_col=None):\n","    \"\"\"Import data from a CSV file and optimize memory usage.\"\"\"\n","    df = pd.read_csv(path, index_col=index_col)\n","    return reduce_mem_usage(df)\n","\n","def reduce_mem_usage(df):\n","    \"\"\"Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\"\"\n","    for col in df.columns:\n","        col_type = df[col].dtype\n","        if isinstance(col_type, pd.IntervalDtype):\n","            continue\n","\n","        if str(col_type)[:3] == 'int':\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                df[col] = df[col].astype(np.int8)\n","            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                df[col] = df[col].astype(np.int16)\n","            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                df[col] = df[col].astype(np.int32)\n","            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                df[col] = df[col].astype(np.int64)  \n","        elif str(col_type)[:5] == 'float':\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                df[col] = df[col].astype(np.float32)\n","            else:\n","                df[col] = df[col].astype(np.float64)\n","    return df\n","\n","def feature_engineering(df):\n","    \"\"\"Feature engineering on the dataset.\"\"\"\n","    # Binning age and converting to categorical labels instead of intervals\n","    age_bins = pd.cut(df['Age'], bins=7, labels=False)\n","    df['Age_Type'] = age_bins\n","    df['Vehicle_Age'] = df['Vehicle_Age'].astype('category').cat.codes\n","    df['Vehicle_Damage'] = df['Vehicle_Damage'].astype('category').cat.codes\n","    df['Previously_Insured'] = df['Previously_Insured'].astype('category').cat.codes\n","\n","    df['Age_x_Vehicle_Age'] = df['Age_Type'] * df['Vehicle_Age']\n","    df['Age_x_Vehicle_Damage'] = df['Age_Type'] * df['Vehicle_Damage']\n","    df['Age_x_Previously_Insured'] = df['Age_Type'] * df['Previously_Insured']\n","\n","    fac_pre = ['Policy_Sales_Channel', 'Vehicle_Damage', 'Annual_Premium', 'Vintage', 'Age_Type']\n","    col_pre = []\n","    for i in fac_pre:\n","        df['Previously_Insured_x_' + i] = pd.factorize(df['Previously_Insured'].astype(str) + df[i].astype(str))[0]\n","        col_pre.append('Previously_Insured_x_' + i)\n","\n","    fac_pro = fac_pre[1:]\n","    col_pro = []\n","    for i in fac_pro:\n","        df['Policy_Sales_Channel_x_' + i] = pd.factorize(df['Policy_Sales_Channel'].astype(str) + df[i].astype(str))[0]\n","        col_pro.append('Policy_Sales_Channel_x_' + i)\n","    return df, col_pre, col_pro\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Load and optimize data\n","train_df = import_data(train_path, index_col='id')\n","test_df = import_data(test_path, index_col='id')\n","\n","# Combine train and test datasets for consistent transformation\n","full_df = pd.concat([train_df, test_df], axis=0)\n","\n","# Convert columns to category type\n","less = ['Gender', 'Vehicle_Age', 'Vehicle_Damage', 'Policy_Sales_Channel']\n","for col in less:\n","    full_df[col] = full_df[col].astype('category')\n","\n","# Apply feature engineering to the combined dataset\n","full_df, col_pre, col_pro = feature_engineering(full_df)\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Split back into train and test sets\n","train_df = full_df.iloc[:len(train_df), :]\n","test_df = full_df.iloc[len(train_df):, :]\n","\n","# Split the training data into training and validation sets\n","X = train_df.drop('Response', axis=1)\n","y = train_df['Response']\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["5348"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Define the ColumnTransformer\n","coltrans = ColumnTransformer(\n","    transformers=[\n","        ('cat', OneHotEncoder(sparse_output=False, dtype=np.float32), ['Gender', 'Vehicle_Damage']),\n","        ('minmax', MinMaxScaler(), ['Age', 'Region_Code', 'Previously_Insured', 'Policy_Sales_Channel', 'Vintage']),\n","        ('ordinal', OrdinalEncoder(categories=[[0, 1, 2]], dtype=np.float32), ['Vehicle_Age']),\n","        ('robust', RobustScaler(), ['Annual_Premium']),\n","        ('standard', StandardScaler(), ['Age_Type', 'Age_x_Vehicle_Age', 'Age_x_Vehicle_Damage', 'Age_x_Previously_Insured']),\n","        ('standard_2', StandardScaler(), col_pre + col_pro),\n","    ],\n","    remainder='passthrough'  # Keeps columns not specified in transformers\n",")\n","\n","# Fit the transformer on the training data and transform both training and validation sets\n","X_train = coltrans.fit_transform(X_train)\n","X_valid = coltrans.transform(X_valid)\n","test_df = coltrans.transform(test_df.drop('Response', axis=1))\n","\n","gc.collect()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training fold 1\n","[0]\ttrain-auc:0.85702\tvalid-auc:0.85723\n","[100]\ttrain-auc:0.87717\tvalid-auc:0.87674\n","[200]\ttrain-auc:0.87967\tvalid-auc:0.87861\n","[300]\ttrain-auc:0.88064\tvalid-auc:0.87896\n","[400]\ttrain-auc:0.88121\tvalid-auc:0.87907\n","[450]\ttrain-auc:0.88144\tvalid-auc:0.87908\n","Training fold 2\n","[0]\ttrain-auc:0.85697\tvalid-auc:0.85713\n","[100]\ttrain-auc:0.87718\tvalid-auc:0.87670\n","[200]\ttrain-auc:0.87972\tvalid-auc:0.87853\n","[300]\ttrain-auc:0.88058\tvalid-auc:0.87884\n","[367]\ttrain-auc:0.88100\tvalid-auc:0.87892\n","Training fold 3\n","[0]\ttrain-auc:0.85705\tvalid-auc:0.85733\n","[100]\ttrain-auc:0.87702\tvalid-auc:0.87659\n","[200]\ttrain-auc:0.87963\tvalid-auc:0.87842\n","[300]\ttrain-auc:0.88063\tvalid-auc:0.87881\n","[400]\ttrain-auc:0.88126\tvalid-auc:0.87892\n","[486]\ttrain-auc:0.88168\tvalid-auc:0.87896\n","Training fold 4\n","[0]\ttrain-auc:0.85726\tvalid-auc:0.85628\n","[100]\ttrain-auc:0.87728\tvalid-auc:0.87580\n","[200]\ttrain-auc:0.87986\tvalid-auc:0.87778\n","[300]\ttrain-auc:0.88080\tvalid-auc:0.87818\n","[400]\ttrain-auc:0.88141\tvalid-auc:0.87829\n","[429]\ttrain-auc:0.88155\tvalid-auc:0.87830\n","Training fold 5\n","[0]\ttrain-auc:0.85706\tvalid-auc:0.85673\n","[100]\ttrain-auc:0.87705\tvalid-auc:0.87629\n","[200]\ttrain-auc:0.87971\tvalid-auc:0.87830\n","[300]\ttrain-auc:0.88069\tvalid-auc:0.87868\n","[400]\ttrain-auc:0.88124\tvalid-auc:0.87880\n"]}],"source":["from sklearn.feature_selection import RFE\n","\n","# Perform feature selection using RFE with XGBoost as the estimator\n","xgb_params = {\n","    'random_state': 512,\n","    'objective': \"binary:logistic\",\n","    'eval_metric': 'auc',\n","    'max_depth': 8,\n","    'min_child_weight': 12,\n","    'colsample_bytree': 0.5,\n","    'gamma': 0.2,\n","    'learning_rate': 0.09093568107192034,\n","    'subsample': 1.0,\n","    'reg_alpha': 0.0011852827097616767,\n","    'reg_lambda': 1.0735757602378362e-06,\n","    'max_bin': 197818,\n","    'scale_pos_weight': len(train_df[train_df['Response'] == 0]) / len(train_df[train_df['Response'] == 1]),  # Adjust this based on your dataset\n","    'tree_method': 'hist',  # Ensure your environment supports GPU\n","    'device': 'cuda',  # Ensure your environment supports GPU\n","}\n","\n","estimator = xgb.XGBClassifier(**xgb_params)\n","selector = RFE(estimator, n_features_to_select=19, step=1)\n","selector = selector.fit(X_train, y_train)\n","\n","# Transform the training, validation, and test data to keep only selected features\n","X_train_selected = selector.transform(X_train)\n","X_valid_selected = selector.transform(X_valid)\n","X_test_selected = selector.transform(test_df)\n","\n","# Train XGBoost model with cross-validation on the selected features\n","xgb_preds = []\n","xgb_aucs = []\n","\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","for fold, (train_idx, valid_idx) in enumerate(skf.split(X_train_selected, y_train)):\n","    print(f\"Training fold {fold + 1}\")\n","    X_train_fold, y_train_fold = X_train_selected[train_idx], y_train.iloc[train_idx]\n","    X_valid_fold, y_valid_fold = X_train_selected[valid_idx], y_train.iloc[valid_idx]\n","    \n","    dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold)\n","    dvalid = xgb.DMatrix(X_valid_fold, label=y_valid_fold)\n","    \n","    model = xgb.train(\n","        xgb_params,\n","        dtrain,\n","        num_boost_round=3000,\n","        evals=[(dtrain, 'train'), (dvalid, 'valid')],\n","        verbose_eval=100,\n","        early_stopping_rounds=10,\n","    )\n","    \n","    valid_preds = model.predict(dvalid, iteration_range=(0, model.best_iteration))\n","    auc_score = roc_auc_score(y_valid_fold, valid_preds)\n","    xgb_aucs.append(auc_score)\n","    \n","    dtest = xgb.DMatrix(X_test_selected)\n","    test_pred = model.predict(dtest, iteration_range=(0, model.best_iteration))\n","    xgb_preds.append(test_pred)\n","    \n","    # Save the model for this fold\n","    model.save_model(f'xgb_model_fold_{fold + 1}.json')\n","    \n","    # Clear memory\n","    del X_train_fold, y_train_fold, X_valid_fold, y_valid_fold, dtrain, dvalid, model\n","    gc.collect()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["['test_pred_xgb2.pkl']"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Average the predictions from each fold for XGBoost\n","test_pred_xgb = np.mean(xgb_preds, axis=0)\n","joblib.dump(test_pred_xgb, 'test_pred_xgb2.pkl')\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Reimport the test_df to get the original index\n","test_df = import_data(test_path, index_col='id')\n","\n","# Create a submission DataFrame using the original test index\n","submission = pd.DataFrame({\n","    'id': test_df.index,\n","    'Response': test_pred_xgb\n","})\n","\n","# Save the submission DataFrame to a CSV file\n","submission.to_csv('submission_xgb2.csv', index=False)\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8930475,"sourceId":73291,"sourceType":"competition"},{"sourceId":189806873,"sourceType":"kernelVersion"},{"sourceId":189806878,"sourceType":"kernelVersion"},{"sourceId":189813334,"sourceType":"kernelVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
