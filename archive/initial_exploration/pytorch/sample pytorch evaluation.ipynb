{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sample sizes...\n",
      "Processing sample size: 2944827\n",
      "Training epoch 1/2 for sample size 2944827\n",
      "Sample Size: 2944827, Epoch 1/2, ROC AUC Score: 0.8684\n",
      "Training epoch 2/2 for sample size 2944827\n",
      "Sample Size: 2944827, Epoch 2/2, ROC AUC Score: 0.8693\n",
      "Processing sample size: 3631793\n",
      "Training epoch 1/2 for sample size 3631793\n",
      "Sample Size: 3631793, Epoch 1/2, ROC AUC Score: 0.8688\n",
      "Training epoch 2/2 for sample size 3631793\n",
      "Sample Size: 3631793, Epoch 2/2, ROC AUC Score: 0.8689\n",
      "Processing sample size: 4318759\n",
      "Training epoch 1/2 for sample size 4318759\n",
      "Sample Size: 4318759, Epoch 1/2, ROC AUC Score: 0.8666\n",
      "Training epoch 2/2 for sample size 4318759\n",
      "Sample Size: 4318759, Epoch 2/2, ROC AUC Score: 0.8677\n",
      "Processing sample size: 5005726\n",
      "Training epoch 1/2 for sample size 5005726\n",
      "Sample Size: 5005726, Epoch 1/2, ROC AUC Score: 0.8683\n",
      "Training epoch 2/2 for sample size 5005726\n",
      "Sample Size: 5005726, Epoch 2/2, ROC AUC Score: 0.8686\n",
      "Processing sample size: 5692692\n",
      "Training epoch 1/2 for sample size 5692692\n",
      "Sample Size: 5692692, Epoch 1/2, ROC AUC Score: 0.8686\n",
      "Training epoch 2/2 for sample size 5692692\n",
      "Sample Size: 5692692, Epoch 2/2, ROC AUC Score: 0.8686\n",
      "Processing sample size: 6379659\n",
      "Training epoch 1/2 for sample size 6379659\n",
      "Sample Size: 6379659, Epoch 1/2, ROC AUC Score: 0.8686\n",
      "Training epoch 2/2 for sample size 6379659\n",
      "Sample Size: 6379659, Epoch 2/2, ROC AUC Score: 0.8689\n",
      "Processing sample size: 7066625\n",
      "Training epoch 1/2 for sample size 7066625\n",
      "Sample Size: 7066625, Epoch 1/2, ROC AUC Score: 0.8689\n",
      "Training epoch 2/2 for sample size 7066625\n",
      "Sample Size: 7066625, Epoch 2/2, ROC AUC Score: 0.8694\n",
      "Processing sample size: 7753592\n",
      "Training epoch 1/2 for sample size 7753592\n",
      "Sample Size: 7753592, Epoch 1/2, ROC AUC Score: 0.8687\n",
      "Training epoch 2/2 for sample size 7753592\n",
      "Sample Size: 7753592, Epoch 2/2, ROC AUC Score: 0.8687\n",
      "Processing sample size: 8440558\n",
      "Training epoch 1/2 for sample size 8440558\n",
      "Sample Size: 8440558, Epoch 1/2, ROC AUC Score: 0.8689\n",
      "Training epoch 2/2 for sample size 8440558\n",
      "Sample Size: 8440558, Epoch 2/2, ROC AUC Score: 0.8688\n",
      "Processing sample size: 9127525\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'test_size' parameter of train_test_split must be a float in the range (0.0, 1.0), an int in the range [1, inf) or None. Got 0.0 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 118\u001b[0m\n\u001b[0;32m    116\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating sample sizes...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating sample sizes...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 118\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_sample_sizes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mResponse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m, in \u001b[0;36mevaluate_sample_sizes\u001b[1;34m(df, target, sample_sizes, epochs, random_state)\u001b[0m\n\u001b[0;32m     45\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing sample size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing sample size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m sample_df, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m X \u001b[38;5;241m=\u001b[39m sample_df\u001b[38;5;241m.\u001b[39mdrop(target, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     50\u001b[0m y \u001b[38;5;241m=\u001b[39m sample_df[target]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\paulo\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:203\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    201\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 203\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\paulo\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'test_size' parameter of train_test_split must be a float in the range (0.0, 1.0), an int in the range [1, inf) or None. Got 0.0 instead."
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, filename='sample_size_analysis.log', filemode='w', \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('transformed__train_dataframe.csv')\n",
    "logger.info(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "# Simplified Neural Network Model\n",
    "class SimplifiedNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimplifiedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # No sigmoid here\n",
    "        return x\n",
    "\n",
    "# Function to Evaluate Sample Sizes\n",
    "def evaluate_sample_sizes(df, target, sample_sizes, epochs=2, random_state=42):\n",
    "    results = []\n",
    "    input_dim = df.drop(target, axis=1).shape[1]\n",
    "    \n",
    "    for sample_size in sample_sizes:\n",
    "        logger.info(f\"Processing sample size: {sample_size}\")\n",
    "        print(f\"Processing sample size: {sample_size}\")\n",
    "        sample_df, _ = train_test_split(df, stratify=df[target], test_size=(1 - sample_size / len(df)), random_state=random_state)\n",
    "        \n",
    "        X = sample_df.drop(target, axis=1).values\n",
    "        y = sample_df[target].values\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        tensor_x_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "        tensor_y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "        tensor_x_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "        tensor_y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "        train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
    "        val_dataset = TensorDataset(tensor_x_val, tensor_y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        model = SimplifiedNN(input_dim).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        scaler = GradScaler()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            logger.info(f\"Training epoch {epoch+1}/{epochs} for sample size {sample_size}\")\n",
    "            print(f\"Training epoch {epoch+1}/{epochs} for sample size {sample_size}\")\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels.unsqueeze(1))\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            all_labels = []\n",
    "            all_outputs = []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    all_outputs.extend(outputs.cpu().numpy())\n",
    "\n",
    "            roc_auc = roc_auc_score(all_labels, all_outputs)\n",
    "            logger.info(f'Sample Size: {sample_size}, Epoch {epoch+1}/{epochs}, ROC AUC Score: {roc_auc:.4f}')\n",
    "            print(f'Sample Size: {sample_size}, Epoch {epoch+1}/{epochs}, ROC AUC Score: {roc_auc:.4f}')\n",
    "        \n",
    "        results.append((sample_size, roc_auc))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Define Sample Sizes to Test (starting from the most meaningful size)\n",
    "start_size = 2944827\n",
    "end_size = len(df)\n",
    "sample_sizes = np.linspace(start_size, end_size, 10, dtype=int)\n",
    "\n",
    "# Evaluate and Find Ideal Sample Size\n",
    "logger.info(\"Evaluating sample sizes...\")\n",
    "print(\"Evaluating sample sizes...\")\n",
    "results = evaluate_sample_sizes(df, 'Response', sample_sizes, epochs=2)\n",
    "logger.info(\"Evaluation completed.\")\n",
    "print(\"Evaluation completed.\")\n",
    "\n",
    "# Convert Results to DataFrame for Easy Analysis\n",
    "results_df = pd.DataFrame(results, columns=['Sample Size', 'ROC AUC Score'])\n",
    "print(results_df)\n",
    "\n",
    "# Plot Results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_df['Sample Size'], results_df['ROC AUC Score'], marker='o')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.title('Sample Size vs. ROC AUC Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
