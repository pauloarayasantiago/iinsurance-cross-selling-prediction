{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73291,"databundleVersionId":8930475,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preliminary Tests and Findings\n\nThis notebook is a compilation of my favorite parts of many notebooks I created for this competition. It might not have been the best performing, but I picked this one because it had my favorite parts of all my attempts. Actually, it is one of at least 40 notebooks I created for this project. One of my biggest challenges was that I wasn't able to run the code efficiently in the cloud, so I had to use my own PC to fit and predict the models. Therefore, I settled for this 1% sample version as a showcase. The purpose of this notebook is to demonstrate my workflow and interpretation of each part.\n\n## Early Explorations\n\nI first started with a little exploratory data analysis (EDA) and basic data processing. The data comes pretty clean, but the dataset is MASSIVE. The largest I had worked with previously had been around 70k rows, but this one had 11 million. Not many columns though. Most columns were fairly easy to handle except for `Region_Code` and `Policy_Sales_Channel`. I treated those by binning them into a rare category due to the heavy imbalance towards some values. Otherwise, everything was treated pretty basically.\n\nI made some basic EDA graphs to explore the data, using some base models to understand feature importances. Later, I discovered the magic of KLIB from another notebook: [Optuna XGBoost KLIB Notebook](https://www.kaggle.com/code/suvroo/ps4e7-optuna-xgboost-klib), which taught me a thing or two about cleaning the data easily with KLIB, and how to keep track of hyperparameter studies with Optuna and some of its also amazing graphs.\n\n## Feature Engineering\n\nAt first I applied the basic preprocessing steps neccessary to run the dataset efficiently in the models I was using. Also incorporating some downcasting to the workflow so that the model would work more efficiently. I tried applying some additional transformations to the data like removing outliers, creating rare categories for feature values with low counts and creating KMEANS cluster feature. I also tested borrowed feature interactions from other notebooks and created my own. After possibly hundreds of tests I settled on a combination of my own outlier removal using IQR and a set of feature interactions from [this Kaggle notebook](https://www.kaggle.com/code/rohanrao/automl-grand-prix-1st-place-solution).\n\n## Hyperparameter Studies\n\nAfter doing hyperparameter studies on LightGBM, XGBoost, CatBoost, and a PyTorch neural network, I realized, while cleaning up my files, that one of my base models had the best metrics so far. That is the one I built this model on. I later implemented Optuna with careful logging of the studies and ran hundreds of different iterations, mixing hyperparameters and different sample sizes to understand how the library worked.\n","metadata":{}},{"cell_type":"markdown","source":"#### Libraries Import\n\nIn this section, we import all necessary libraries required for data manipulation, visualization, model building, and evaluation.","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install klib","metadata":{"execution":{"iopub.status.busy":"2024-07-23T03:56:16.263468Z","iopub.execute_input":"2024-07-23T03:56:16.263898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport klib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport optuna\nfrom datetime import datetime\nimport lightgbm as lgb\nimport os\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Loading\n\nHere, we load the training dataset. This dataset will be used for all subsequent data processing and model training steps. This dataset was created artificially for the Kaggle Playground Series S4E7, based on [this set](https://www.kaggle.com/datasets/annantkumarsingh/health-insurance-cross-sell-prediction-data).\n","metadata":{}},{"cell_type":"code","source":"# Load the datasets\ntrain_df = pd.read_csv(\"/kaggle/input/playground-series-s4e7/train.csv\", index_col='id')\ntest_df = pd.read_csv(\"/kaggle/input/playground-series-s4e7/test.csv\", index_col='id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Stratified Sampling\n\nSince the target variable is heavily imbalanced, we perform stratified sampling to ensure that the sample maintains the same distribution as the full dataset. Due to the large size of the dataset (11 million rows), we use only 1% for this notebook. In this case I used %40, because I determined in aother notebook that was the ideal size of a sample in order to approximate the performance of using the full dataset.\n","metadata":{}},{"cell_type":"code","source":"# Separate features and target variable\nX = train_df.drop('Response', axis=1)\ny = train_df['Response']\n\n# # Determine sample size (10% of the dataset)\n# sample_size = 0.01\n\n# # Stratified sampling\n# X_sample, _, y_sample, _ = train_test_split(X, y, train_size=sample_size, stratify=y)\n\n# # Combine sampled features and target variable\n# train_df = pd.concat([X_sample, y_sample], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Display Sampled Data\n\nAfter performing stratified sampling, we display the first few rows of the sampled dataset to understand its structure and verify the sampling process.\n","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Summary\n\nWe use the `describe` method to generate summary statistics for the numerical columns in the dataset. This provides insights into the central tendency, dispersion, and shape of the datasetâ€™s distribution.\n","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Missing Values Check\n\nIt is essential to check for missing values in the dataset as they can affect the model performance. Here, we count the number of missing values in each column.\n","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Initial Observations and Ideas\n\n- **Gender, Driving_License, Regional_Code, Previously_Insured, Vehicle_Age, Vehicle_Damage, Policy_Sales_Channel, and Response** are all categories. I will treat most of them as numerical columns for now, except for Gender, Vehicle_Age, Previously_Insured, and Vehicle_Damage, which I will turn into categories to use in KLIB's streamlined categorical plotting. From previous explorations, I know Driving_License only has 1 negative value, so I will drop it.\n- I will later remap those four categories into numerical columns after EDA as part of the preprocessing for the model.\n- I will MinMax scale Age and Vintage when standardizing because they have a reasonable range for this type of transformation.\n","metadata":{}},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"I turn some categories I want to plot into category dtype to be compatible with klib, they will automatically switch back to numeric during preprocessing. ","metadata":{}},{"cell_type":"code","source":"# Convert specified columns to categorical\ncategorical_columns = ['Gender', 'Vehicle_Age', 'Previously_Insured']\n\nfor col in categorical_columns:\n    train_df[col] = train_df[col].astype('category')\n\n# Convert 'Previously_Insured' column to a categorical type with specific labels\ntrain_df['Previously_Insured'] = pd.Categorical(train_df['Previously_Insured'], categories=[0, 1], ordered=True)\ntrain_df['Previously_Insured'] = train_df['Previously_Insured'].cat.rename_categories([\"Uninsured\", \"Insured\"])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualizing Data with KLIB\n\nUsing KLIB, we create categorical plots to visualize the distribution of categorical features in the dataset. This helps in understanding the balance of different categories within the dataset.","metadata":{}},{"cell_type":"code","source":"klib.cat_plot(train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### KLIB Cat Plot Explanation\n\nThe KLIB categorical plot is an interesting way of visualizing binary variables within a dataset. However, it doesn't translate too well with categorical variables having more than two possible values.\n","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Function to plot the relationship between categorical variables and the target in a 2x2 single figure\ndef plot_categorical_vs_target(df, cat_cols, target_col):\n    num_plots = len(cat_cols)\n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))\n    palette = [\"#66c2a5\", \"#fc8d62\"]  # Custom palette with exactly two colors\n\n    for ax, col in zip(axes.flatten(), cat_cols):\n        sns.countplot(data=df, x=col, hue=target_col, ax=ax, palette=palette)\n        ax.set_title(f'Relation between {col} and {target_col}')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plot the relationships\ncategorical_columns = ['Gender', 'Vehicle_Age', 'Previously_Insured', 'Vehicle_Damage']\nplot_categorical_vs_target(train_df, categorical_columns, 'Response')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The binary categories were balanced for the most part. Gender's relation to the target doesn't tell us much knowing that there are slightly more Males to begin with. People with newer vehicles are much more likely to insure them. Naturally people that are already insured answered No (who knows if that was a no to switching providers as well). People with no vehicle damage are not very likely to ensure their cars.","metadata":{}},{"cell_type":"markdown","source":"#### Distribution Plots with KLIB\n\nWe use KLIB to create distribution plots for several features, including Annual_Premium, Age, Region_Code, Policy_Sales_Channel, and Vintage. This helps in understanding the distribution and identifying potential outliers.","metadata":{}},{"cell_type":"code","source":"# Plotting categorical features against the target variable\nklib.dist_plot(train_df[['Annual_Premium']])\nklib.dist_plot(train_df[['Age']])\nklib.dist_plot(train_df[['Region_Code']])\nklib.dist_plot(train_df[['Policy_Sales_Channel']])\nklib.dist_plot(train_df[['Vintage']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Distribution Plot Analysis\n\n- **Annual_Premium** is heavily right-skewed with some heavy outliers in larger numbers. It is also bimodally distributed with a large concentration in the lower values and a second concentration near the mean. I might use a standard outlier removal method or a more aggressive quartile method. It might also benefit from log-transformation for its skewness.\n- **Age** is right-skewed, but given the nature of the feature, I will leave it as is and use only a MinMax Scaler during preprocessing.\n- **Region_Code** is actually a category but has many different values. It is clear certain regions are much more favored. In a previous notebook, I tried compiling all of the rarer codes into their own category. I might attempt this again but will need to consider that the rare category must be a number in itself for the model.\n- **Policy_Sales_Channel** is similar to Region_Code and will be treated the same way.\n- **Vintage** is the most normally distributed feature and I will probably scale it with a MinMax Scaler.\n","metadata":{}},{"cell_type":"markdown","source":"#### Encoding Categorical Features\n\nTo prepare the categorical features for model training, we convert them into numerical codes. Here, we encode the 'Previously_Insured' feature.","metadata":{}},{"cell_type":"code","source":"train_df['Previously_Insured'] = train_df['Previously_Insured'].cat.codes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Correlation Plot with KLIB\n\nWe use KLIB to create a correlation plot to identify relationships between the features and the target variable. This helps in understanding which features might be important for the model.","metadata":{}},{"cell_type":"code","source":"klib.corr_plot(train_df, target='Response')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Correlation Plot Analysis\n\nThe only feature that seems to have a significant relation towards conversion on the target is whether people are uninsured prior to the call.","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing Data\n\nIn this step, we preprocess the data by transforming binary variables, encoding ordinal features, and scaling continuous features. This prepares the dataset for model training.\n","metadata":{}},{"cell_type":"code","source":"# Preprocess data\ndef preprocess_data(df):\n    # Transform binary variables\n    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n    df['Vehicle_Damage'] = df['Vehicle_Damage'].map({'Yes': 1, 'No': 0})\n    \n    # Ordinal Encoding for Vehicle_Age\n    vehicle_age_mapping = {'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2}\n    df['Vehicle_Age'] = df['Vehicle_Age'].map(vehicle_age_mapping)\n    \n    # Drop Driving_License due to limited variability\n    df.drop(['Driving_License'], axis=1, inplace=True)\n    \n    # Min-Max Scaling for Age and Vintage\n    df['Age'] = (df['Age'] - df['Age'].min()) / (df['Age'].max() - df['Age'].min())\n    df['Vintage'] = (df['Vintage'] - df['Vintage'].min()) / (df['Vintage'].max() - df['Vintage'].min())\n    \n    return df\n\ntrain_df = preprocess_data(train_df)\ntest_df = preprocess_data(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"#### Converting Data Types\n\nHere, we convert the data types of certain columns to integers for consistency and efficiency.\n","metadata":{}},{"cell_type":"code","source":"train_df[['Gender', 'Vehicle_Age', 'Vehicle_Damage']] = train_df[['Gender', 'Vehicle_Age', 'Vehicle_Damage']].astype('int')\ntrain_df[['Region_Code', 'Annual_Premium', 'Policy_Sales_Channel']] = train_df[['Region_Code', 'Annual_Premium', 'Policy_Sales_Channel']].astype('int')\ntest_df[['Gender', 'Vehicle_Age', 'Vehicle_Damage']] = test_df[['Gender', 'Vehicle_Age', 'Vehicle_Damage']].astype('int')\ntest_df[['Region_Code', 'Annual_Premium', 'Policy_Sales_Channel']] = test_df[['Region_Code', 'Annual_Premium', 'Policy_Sales_Channel']].astype('int')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Removing Outliers\n\nWe remove outliers from the 'Annual_Premium' feature using the Interquartile Range (IQR) method to ensure the model is not affected by extreme values.\n","metadata":{}},{"cell_type":"code","source":"# # Remove outliers from Annual_Premium in training data only\n# def remove_outliers_iqr(df, column):\n#     Q1 = df[column].quantile(0.25)\n#     Q3 = df[column].quantile(0.75)\n#     IQR = Q3 - Q1\n#     lower_bound = Q1 - 1.5 * IQR\n#     upper_bound = Q3 + 1.5 * IQR\n    \n#     return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n\n# train_df = remove_outliers_iqr(train_df, 'Annual_Premium')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Reference: https://www.kaggle.com/code/rohanrao/automl-grand-prix-1st-place-solution\nWe create new features by capturing the interactions between them and encoding that into categorical combinations. This can provide additional information to the model and improve its performance.\n","metadata":{}},{"cell_type":"code","source":"def feature_engineering(df):\n    df = df.copy()  \n    df['Previously_Insured_Annual_Premium'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Annual_Premium'].astype(str)))[0]\n    df['Previously_Insured_Vehicle_Age'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Vehicle_Age'].astype(str)))[0]\n    df['Previously_Insured_Vehicle_Damage'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Vehicle_Damage'].astype(str)))[0]\n    df['Previously_Insured_Vintage'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Vintage'].astype(str)))[0]\n\n    return df\n\ntrain_df = feature_engineering(train_df)\ntest_df = feature_engineering(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Downcasting","metadata":{}},{"cell_type":"markdown","source":"\n\nTo optimize memory usage, we downcast numerical columns to more appropriate data types without losing information.\n","metadata":{}},{"cell_type":"code","source":"def optimize_dtypes(df):\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            if 'int' in str(col_type):\n                min_val, max_val = df[col].min(), df[col].max()\n                if min_val >= np.iinfo(np.int8).min and max_val <= np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif min_val >= np.iinfo(np.int16).min and max_val <= np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif min_val >= np.iinfo(np.int32).min and max_val <= np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif min_val >= np.iinfo(np.int64).min and max_val <= np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            elif 'float' in str(col_type):\n                min_val, max_val = df[col].min(), df[col].max()\n                if min_val >= np.finfo(np.float16).min and max_val <= np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif min_val >= np.finfo(np.float32).min and max_val <= np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                elif min_val >= np.finfo(np.float64).min and max_val <= np.finfo(np.float64).max:\n                    df[col] = df[col].astype(np.float64)\n    \n    return df\n\ntrain_df = optimize_dtypes(train_df)\ntest_df = optimize_dtypes(test_df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"\nWe use Optuna to perform hyperparameter tuning for our model. This helps in finding the best set of hyperparameters to improve model performance.\n","metadata":{}},{"cell_type":"markdown","source":"#### Separating Features and Target Variable\n\nWe separate the features (X) and the target variable (y) from the preprocessed dataset. This is an essential step before model training.\n","metadata":{}},{"cell_type":"code","source":"# Separate features and target variable\nX = train_df.drop('Response', axis=1)\ny = train_df['Response'] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Scaling\n\nWe apply standard scaling to the features to ensure they are on a similar scale. This is important for many machine learning algorithms to perform well.\n","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\ntrain_df =  scaler.fit_transform(X)\ntest_df =  pd.DataFrame(scaler.transform(test_df), index=test_df.index, columns=test_df.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final Model Training\n\nWe train the final model using the best hyperparameters found using optuna oof and evaluate its performance on the test set.\n","metadata":{}},{"cell_type":"code","source":"%%capture\nparam = {\n         'reg_alpha': 0.03432385172267505, \n         'reg_lambda': 0.2998279059616829, \n         'colsample_bytree': 0.790292183596673, \n         'subsample': 0.9046878168822107, \n         'learning_rate': 0.05035039561309864, \n         'max_depth': 29, \n         'num_leaves': 1474, \n         'min_child_samples': 75, \n         'min_child_weight': 7.661448090878849, \n         'min_split_gain': 0.09978597066868167, \n         'max_bin': 499, \n         'scale_pos_weight': 9.870717062897523, \n         'num_boost_round': 17349\n        }\n\nnum_boost_round = 20000\n\n# Create LightGBM dataset\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n    \n# Train model\nbst = lgb.train(\n    param,\n    train_data, \n    valid_sets=[valid_data],\n    num_boost_round=num_boost_round,\n    )\n    \n# Predict and evaluate\ny_val_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\nauc = roc_auc_score(y_val, y_val_pred)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions\ny_train_pred_proba = bst.predict(X_train, num_iteration=bst.best_iteration)\ny_val_pred_proba = bst.predict(X_val, num_iteration=bst.best_iteration)\n\n# Calculate ROC AUC scores\nroc_auc_train = roc_auc_score(y_train, y_train_pred_proba)\nroc_auc_val = roc_auc_score(y_val, y_val_pred_proba)\n\n# Print ROC AUC scores\nprint(f'Training ROC AUC Score: {roc_auc_train}')\nprint(f'Validation ROC AUC Score: {roc_auc_val}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission\ntest_predictions = bst.predict(test_df, num_iteration=bst.best_iteration)\nsubmission = pd.DataFrame({'id': test_df.index, 'Response': test_predictions})\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"Submission file created successfully.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\n\nIn this notebook, we performed exploratory data analysis, preprocessing, feature engineering, and model training with hyperparameter tuning. The final model was trained using LightGBM with the best hyperparameters found by Optuna and evaluated on the test set. The results demonstrate the effectiveness of the selected features and the tuned model. This notebook was designed to showcase a clean and optimized workflow, but doesn't necessarily grant the best results for this competition.\n","metadata":{}},{"cell_type":"markdown","source":"## Techniques and Strategies Attempted\n\n### Techniques for Balancing the Dataset\n\nThe target variable was highly imbalanced, so I used SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset. This technique generates synthetic samples for the minority class, but in this case in generated way too much overfitting.\n\n## Models\n\n### List and Description of Models Attempted\n\nI tried out several models:\n\n1. **Logistic Regression:** A solid baseline for binary classification.\n2. **Decision Trees:** Simple and good for capturing non-linear relationships.\n3. **Random Forests:** An ensemble method that improved decision tree performance.\n4. **Gradient Boosting (XGBoost and LightGBM):** Advanced ensemble methods that handled the large dataset and boosted accuracy.\n5. **Neural Networks:** For capturing more complex patterns in the data.\n6. **Autoencoders:** These were great for feature extraction and dimensionality reduction.\n\n## Challenges and Solutions\n\n- **Handling Large Datasets:** The 11 million rows were daunting. I downsampled the dataset for initial exploration to manage computational resources better.\n- **Imbalanced Dataset:** SMote was my go-to solution for balancing the target variable classes. SMote resulted in too much overfitting.\n- **Hyperparameter Tuning:** Given the computational constraints, I leaned heavily on Optuna for efficient hyperparameter tuning. Hyperparameter tuning took almost two weeks without many gains.","metadata":{}},{"cell_type":"markdown","source":"## References\n\nWalter Reade, & Ashley Chow. (2024). *Binary Classification of Insurance Cross Selling*. Kaggle. Retrieved from [https://kaggle.com/competitions/playground-series-s4e7](https://kaggle.com/competitions/playground-series-s4e7)\n","metadata":{}}]}