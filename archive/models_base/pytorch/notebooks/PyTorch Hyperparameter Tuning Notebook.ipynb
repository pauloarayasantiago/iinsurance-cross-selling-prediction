{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-11 00:55:28,944] A new study created in memory with name: no-name-4a57aba3-a16a-468f-9e0c-869247892c37\n",
      "[I 2024-07-11 01:56:04,597] Trial 0 finished with value: 0.8535028001969027 and parameters: {'hidden_dim1': 197, 'hidden_dim2': 79, 'dropout_rate': 0.311631420927649, 'learning_rate': 0.0038242493070039844}. Best is trial 0 with value: 0.8535028001969027.\n",
      "[I 2024-07-11 02:56:56,897] Trial 1 finished with value: 0.855951698884446 and parameters: {'hidden_dim1': 123, 'hidden_dim2': 54, 'dropout_rate': 0.3591830556266409, 'learning_rate': 0.001593215219241058}. Best is trial 1 with value: 0.855951698884446.\n",
      "[I 2024-07-11 03:58:22,534] Trial 2 finished with value: 0.8535594970794276 and parameters: {'hidden_dim1': 151, 'hidden_dim2': 89, 'dropout_rate': 0.1385585250764129, 'learning_rate': 0.0052568783848872735}. Best is trial 1 with value: 0.855951698884446.\n",
      "[I 2024-07-11 05:00:26,061] Trial 3 finished with value: 0.8374504152727854 and parameters: {'hidden_dim1': 154, 'hidden_dim2': 89, 'dropout_rate': 0.4795429772724853, 'learning_rate': 0.008846306746347404}. Best is trial 1 with value: 0.855951698884446.\n",
      "[I 2024-07-11 06:01:31,677] Trial 4 finished with value: 0.8558042696757711 and parameters: {'hidden_dim1': 212, 'hidden_dim2': 47, 'dropout_rate': 0.20655567508109096, 'learning_rate': 0.0028373236649830015}. Best is trial 1 with value: 0.855951698884446.\n",
      "[I 2024-07-11 07:01:55,783] Trial 5 finished with value: 0.8339038560921974 and parameters: {'hidden_dim1': 250, 'hidden_dim2': 83, 'dropout_rate': 0.2983791636926736, 'learning_rate': 0.009924890635773061}. Best is trial 1 with value: 0.855951698884446.\n",
      "[I 2024-07-11 08:01:57,958] Trial 6 finished with value: 0.8489545166471634 and parameters: {'hidden_dim1': 172, 'hidden_dim2': 46, 'dropout_rate': 0.4802500768745399, 'learning_rate': 0.004831573245938801}. Best is trial 1 with value: 0.855951698884446.\n",
      "[I 2024-07-11 09:01:35,584] Trial 7 finished with value: 0.8473605711001551 and parameters: {'hidden_dim1': 76, 'hidden_dim2': 57, 'dropout_rate': 0.35921907108572526, 'learning_rate': 0.004255077263992395}. Best is trial 1 with value: 0.855951698884446.\n",
      "[I 2024-07-11 10:01:00,123] Trial 8 finished with value: 0.85100260380474 and parameters: {'hidden_dim1': 216, 'hidden_dim2': 97, 'dropout_rate': 0.31483902005748476, 'learning_rate': 0.0035510259630248146}. Best is trial 1 with value: 0.855951698884446.\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Hyperparameter Tuning Notebook\n",
    "\n",
    "# Import Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Setting up the logger\n",
    "logging.basicConfig(level=logging.INFO, filename='pytorch_hyperparameter_tuning.log', filemode='w',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Create directories for storing logs and graphs\n",
    "os.makedirs('graphs_pytorch_tuning', exist_ok=True)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Load Data\n",
    "train_path = \"train_pytorch_processed.csv\"\n",
    "test_path = \"test_pytorch_processed.csv\"\n",
    "\n",
    "logger.info(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "logger.info(\"Datasets loaded successfully.\")\n",
    "logger.info(f\"Train dataset shape: {train_df.shape}\")\n",
    "logger.info(f\"Test dataset shape: {test_df.shape}\")\n",
    "\n",
    "# Use a 40% sample of the training data\n",
    "logger.info(\"Sampling 40% of the training data...\")\n",
    "train_sample = train_df.sample(frac=0.4, random_state=42)\n",
    "logger.info(f\"Train sample shape: {train_sample.shape}\")\n",
    "\n",
    "# Split data into features and target\n",
    "X = train_sample.drop('Response', axis=1)\n",
    "y = train_sample['Response']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Define the PyTorch dataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Define the DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, dropout_rate):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    hidden_dim1 = trial.suggest_int('hidden_dim1', 64, 256)\n",
    "    hidden_dim2 = trial.suggest_int('hidden_dim2', 32, 128)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2)\n",
    "    \n",
    "    # Create the model\n",
    "    model = SimpleNN(X.shape[1], hidden_dim1, hidden_dim2, dropout_rate).to(device)\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    # Calculate ROC AUC score\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    return roc_auc\n",
    "\n",
    "# Set up Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "logger.info(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "hidden_dim1 = best_params['hidden_dim1']\n",
    "hidden_dim2 = best_params['hidden_dim2']\n",
    "dropout_rate = best_params['dropout_rate']\n",
    "learning_rate = best_params['learning_rate']\n",
    "\n",
    "final_model = SimpleNN(X.shape[1], hidden_dim1, hidden_dim2, dropout_rate).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop for the final model\n",
    "num_epochs = 10\n",
    "final_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Validation loop for the final model\n",
    "final_model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = final_model(X_batch)\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "        y_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "logger.info(f\"Validation ROC AUC Score with best parameters: {roc_auc}\")\n",
    "\n",
    "print(f\"Validation ROC AUC Score with best parameters: {roc_auc}\")\n",
    "\n",
    "# Save the final model\n",
    "torch.save(final_model.state_dict(), 'final_pytorch_model.pth')\n",
    "logger.info('Final model saved to final_pytorch_model.pth')\n",
    "\n",
    "# Visualize training loss over epochs\n",
    "plt.plot(range(num_epochs), epoch_loss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.savefig('graphs_pytorch_tuning/training_loss.png')\n",
    "plt.show()\n",
    "logger.info('Training loss plot saved.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
