{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental Tuning with PyTorch Dataset on GPU\n",
    "\n",
    "\n",
    "## Cell 1: Initial Setup\n",
    "\n",
    "\n",
    "# Import Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import logging\n",
    "\n",
    "# Setting up the logger\n",
    "logging.basicConfig(level=logging.INFO, filename='pytorch_initial_tuning.log', filemode='w',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Create directories for storing graphs\n",
    "os.makedirs('graphs_pytorch_incremental', exist_ok=True)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f'Using device: {device}')\n",
    "\n",
    "# Load Data\n",
    "train_path = \"train_pytorch_processed.csv\"\n",
    "test_path = \"test_pytorch_processed.csv\"\n",
    "\n",
    "logger.info(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "logger.info(\"Datasets loaded successfully.\")\n",
    "logger.info(f\"Train dataset shape: {train_df.shape}\")\n",
    "logger.info(f\"Test dataset shape: {test_df.shape}\")\n",
    "\n",
    "# Split data into features and target\n",
    "X = train_df.drop('Response', axis=1).values\n",
    "y = train_df['Response'].values\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42, stratify=y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    input_dim = X.shape[1]\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 16, 128)\n",
    "    output_dim = 1\n",
    "\n",
    "    model = Net(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1)\n",
    "    \n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(10):  # Train for 10 epochs\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                val_outputs = model(X_val_batch)\n",
    "                val_loss += criterion(val_outputs.squeeze(), y_val_batch).item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        trial.report(val_loss, epoch)\n",
    "        \n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-12 13:06:43,001] A new study created in memory with name: no-name-0d968e4b-9f0a-4d98-bba3-9b44978c95e2\n",
      "[I 2024-07-12 15:04:39,550] Trial 0 finished with value: 0.2934769336757443 and parameters: {'hidden_dim': 77, 'optimizer': 'Adam', 'lr': 0.050234535484116695}. Best is trial 0 with value: 0.2934769336757443.\n",
      "[I 2024-07-12 15:36:50,227] Trial 1 finished with value: 0.2913493198532937 and parameters: {'hidden_dim': 123, 'optimizer': 'RMSprop', 'lr': 0.02223232908642494}. Best is trial 1 with value: 0.2913493198532937.\n",
      "[I 2024-07-12 16:08:59,601] Trial 2 finished with value: 0.28680926350104186 and parameters: {'hidden_dim': 57, 'optimizer': 'RMSprop', 'lr': 0.024259345169640407}. Best is trial 2 with value: 0.28680926350104186.\n",
      "[I 2024-07-12 16:39:04,029] Trial 3 finished with value: 0.2636828551073202 and parameters: {'hidden_dim': 119, 'optimizer': 'SGD', 'lr': 0.012298487011468845}. Best is trial 3 with value: 0.2636828551073202.\n",
      "[I 2024-07-12 17:14:04,561] Trial 4 finished with value: 0.29196377101834586 and parameters: {'hidden_dim': 48, 'optimizer': 'Adam', 'lr': 0.052708207613693}. Best is trial 3 with value: 0.2636828551073202.\n",
      "[I 2024-07-12 17:17:36,178] Trial 5 pruned. \n",
      "[I 2024-07-12 17:52:49,724] Trial 6 finished with value: 0.2686593009749215 and parameters: {'hidden_dim': 17, 'optimizer': 'Adam', 'lr': 0.011426166416771334}. Best is trial 3 with value: 0.2636828551073202.\n",
      "[I 2024-07-12 18:23:15,520] Trial 7 finished with value: 0.26375753811310054 and parameters: {'hidden_dim': 93, 'optimizer': 'SGD', 'lr': 0.011114809339093819}. Best is trial 3 with value: 0.2636828551073202.\n",
      "[I 2024-07-12 18:53:44,209] Trial 8 finished with value: 0.26424526278920835 and parameters: {'hidden_dim': 19, 'optimizer': 'SGD', 'lr': 0.09922345437180957}. Best is trial 3 with value: 0.2636828551073202.\n",
      "[I 2024-07-12 19:25:25,766] Trial 9 finished with value: 0.26378712207978317 and parameters: {'hidden_dim': 30, 'optimizer': 'SGD', 'lr': 0.09586294060615497}. Best is trial 3 with value: 0.2636828551073202.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m results_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch_initial_search_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train the final model with the best parameters\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m final_model \u001b[38;5;241m=\u001b[39m Net(\u001b[43minput_dim\u001b[49m, best_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], output_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_dim' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "logger.info(f\"Best parameters from initial search: {best_params}\")\n",
    "logger.info(f\"Best validation loss from initial search: {study.best_value}\")\n",
    "\n",
    "# Save results\n",
    "results_df = study.trials_dataframe()\n",
    "results_df.to_csv('pytorch_initial_search_results.csv', index=False)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_model = Net(input_dim, best_params['hidden_dim'], output_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "if best_params['optimizer'] == 'Adam':\n",
    "    optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "elif best_params['optimizer'] == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(final_model.parameters(), lr=best_params['lr'])\n",
    "else:\n",
    "    optimizer = optim.SGD(final_model.parameters(), lr=best_params['lr'])\n",
    "\n",
    "# Training the final model\n",
    "final_model.train()\n",
    "for epoch in range(10):  # Train for 10 epochs\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(X_batch)\n",
    "        loss = criterion(outputs.squeeze(), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save final model\n",
    "torch.save(final_model.state_dict(), 'final_pytorch_model_initial.pth')\n",
    "logger.info('Final model saved to final_pytorch_model_initial.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update logger\n",
    "logging.basicConfig(level=logging.INFO, filename='pytorch_intermediate_tuning.log', filemode='w',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Adapt ranges based on the initial search results\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', int(best_params['hidden_dim'] * 0.8), int(best_params['hidden_dim'] * 1.2))\n",
    "    lr = trial.suggest_float('lr', best_params['lr'] * 0.8, best_params['lr'] * 1.2)\n",
    "\n",
    "    model = Net(input_dim, hidden_dim, output_dim).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    if best_params['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif best_params['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(10):  # Train for 10 epochs\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                val_outputs = model(X_val_batch)\n",
    "                val_loss += criterion(val_outputs.squeeze(), y_val_batch).item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        trial.report(val_loss, epoch)\n",
    "        \n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "# Set up Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "logger.info(f\"Best parameters from intermediate search: {best_params}\")\n",
    "logger.info(f\"Best validation loss from intermediate search: {study.best_value}\")\n",
    "\n",
    "# Save results\n",
    "results_df = study.trials_dataframe()\n",
    "results_df.to_csv('pytorch_intermediate_search_results.csv', index=False)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_model = Net(input_dim, best_params['hidden_dim'], output_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "if best_params['optimizer'] == 'Adam':\n",
    "    optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "elif best_params['optimizer'] == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(final_model.parameters(), lr=best_params['lr'])\n",
    "else:\n",
    "    optimizer = optim.SGD(final_model.parameters(), lr=best_params['lr'])\n",
    "\n",
    "# Training the final model\n",
    "final_model.train()\n",
    "for epoch in range(10):  # Train for 10 epochs\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(X_batch)\n",
    "        loss = criterion(outputs.squeeze(), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save final model\n",
    "torch.save(final_model.state_dict(), 'final_pytorch_model_intermediate.pth')\n",
    "logger.info('Final model saved to final_pytorch_model_intermediate.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update logger\n",
    "logging.basicConfig(level=logging.INFO, filename='pytorch_fine_tuning.log', filemode='w',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Adapt ranges based on the intermediate search results\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', int(best_params['hidden_dim'] * 0.9), int(best_params['hidden_dim'] * 1.1))\n",
    "    lr = trial.suggest_float('lr', best_params['lr'] * 0.9, best_params['lr'] * 1.1)\n",
    "\n",
    "    model = Net(input_dim, hidden_dim, output_dim).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    if best_params['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif best_params['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(10):  # Train for 10 epochs\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                val_outputs = model(X_val_batch)\n",
    "                val_loss += criterion(val_outputs.squeeze(), y_val_batch).item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        trial.report(val_loss, epoch)\n",
    "        \n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "# Set up Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "logger.info(f\"Best parameters from fine-tuning search: {best_params}\")\n",
    "logger.info(f\"Best validation loss from fine-tuning search: {study.best_value}\")\n",
    "\n",
    "# Save results\n",
    "results_df = study.trials_dataframe()\n",
    "results_df.to_csv('pytorch_fine_tuning_search_results.csv', index=False)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_model = Net(input_dim, best_params['hidden_dim'], output_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "if best_params['optimizer'] == 'Adam':\n",
    "    optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "elif best_params['optimizer'] == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(final_model.parameters(), lr=best_params['lr'])\n",
    "else:\n",
    "    optimizer = optim.SGD(final_model.parameters(), lr=best_params['lr'])\n",
    "\n",
    "# Training the final model\n",
    "final_model.train()\n",
    "for epoch in range(10):  # Train for 10 epochs\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(X_batch)\n",
    "        loss = criterion(outputs.squeeze(), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save final model\n",
    "torch.save(final_model.state_dict(), 'final_pytorch_model_fine_tuned.pth')\n",
    "logger.info('Final model saved to final_pytorch_model_fine_tuned.pth')\n",
    "\n",
    "# Plot feature importances (if applicable)\n",
    "# Note: Neural networks don't have feature importances like tree-based models,\n",
    "# so this step can be omitted or replaced with another model evaluation metric.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
