{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import klib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.cluster import KMeans\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\train.csv\", index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = train_df.drop('Response', axis=1)\n",
    "y = train_df['Response']\n",
    "\n",
    "# Determine sample size (10% of the dataset)\n",
    "sample_size = 0.4\n",
    "\n",
    "# Stratified sampling\n",
    "X_sample, _, y_sample, _ = train_test_split(X, y, train_size=sample_size, stratify=y)\n",
    "\n",
    "# Combine sampled features and target variable\n",
    "train_df = pd.concat([X_sample, y_sample], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "def preprocess_data(df):\n",
    "    # Transform binary variables\n",
    "    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n",
    "    df['Vehicle_Damage'] = df['Vehicle_Damage'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    # Ordinal Encoding for Vehicle_Age\n",
    "    vehicle_age_mapping = {'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2}\n",
    "    df['Vehicle_Age'] = df['Vehicle_Age'].map(vehicle_age_mapping)\n",
    "    \n",
    "    # Drop Driving_License due to limited variability\n",
    "    df.drop(['Driving_License'], axis=1, inplace=True)\n",
    "    \n",
    "    # Min-Max Scaling for Age and Vintage\n",
    "    df['Age'] = (df['Age'] - df['Age'].min()) / (df['Age'].max() - df['Age'].min())\n",
    "    df['Vintage'] = (df['Vintage'] - df['Vintage'].min()) / (df['Vintage'].max() - df['Vintage'].min())\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_data = preprocess_data(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['Gender', 'Vehicle_Age', 'Vehicle_Damage']] = train_df[['Gender', 'Vehicle_Age', 'Vehicle_Damage']].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from Annual_Premium in training data only\n",
    "def remove_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "train_df = remove_outliers_iqr(train_df, 'Annual_Premium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_rare_categories(df, column, threshold=0.01):\n",
    "    category_freq = df[column].value_counts(normalize=True)\n",
    "    rare_categories = category_freq[category_freq < threshold].index\n",
    "    \n",
    "    # Use .loc to avoid SettingWithCopyWarning\n",
    "    df.loc[df[column].isin(rare_categories), column] = 88\n",
    "    \n",
    "    return df\n",
    "\n",
    "categorical = ['Region_Code', 'Policy_Sales_Channel']\n",
    "for col in categorical:\n",
    "    train_df = group_rare_categories(train_df, col, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    df = df.copy()  # Create a copy to avoid the warning\n",
    "    df.loc[:, 'Age_Vehicle_Age'] = df['Age'] * df['Vehicle_Age']\n",
    "    df.loc[:, 'Age_Previously_Insured'] = df['Age'] * df['Previously_Insured']\n",
    "    df.loc[:, 'Vehicle_Age_Damage'] = df['Vehicle_Age'] * df['Vehicle_Damage']\n",
    "    df.loc[:, 'Previously_Insured_Damage'] = df['Previously_Insured'] * df['Vehicle_Damage']\n",
    "    df.loc[:, 'Age_squared'] = df['Age'] ** 2\n",
    "    df.loc[:, 'Vehicle_Age_squared'] = df['Vehicle_Age'] ** 2\n",
    "    df.loc[:, 'Annual_Premium_per_Age'] = df['Annual_Premium'] / (df['Age'] + 1)\n",
    "    return df\n",
    "\n",
    "train_df = feature_engineering(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KMeans clustering\n",
    "optimal_clusters = 4\n",
    "train_df['Cluster'] = KMeans(n_clusters=optimal_clusters, random_state=42).fit_predict(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            if 'int' in str(col_type):\n",
    "                min_val, max_val = df[col].min(), df[col].max()\n",
    "                if min_val >= np.iinfo(np.int8).min and max_val <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif min_val >= np.iinfo(np.int16).min and max_val <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif min_val >= np.iinfo(np.int32).min and max_val <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif min_val >= np.iinfo(np.int64).min and max_val <= np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            elif 'float' in str(col_type):\n",
    "                min_val, max_val = df[col].min(), df[col].max()\n",
    "                if min_val >= np.finfo(np.float16).min and max_val <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif min_val >= np.finfo(np.float32).min and max_val <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif min_val >= np.finfo(np.float64).min and max_val <= np.finfo(np.float64).max:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = optimize_dtypes(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = train_df.drop('Response', axis=1)\n",
    "y = train_df['Response'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_df =  scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'lambda_l1': 0.02,  \n",
    "    'lambda_l2': 0.1,  \n",
    "    'colsample_bytree': 0.75, \n",
    "    'subsample': 0.85, \n",
    "    'learning_rate':  0.09,  \n",
    "    'max_depth': 32, \n",
    "    'num_leaves': 900, \n",
    "    'min_child_samples': 50,  \n",
    "    'min_child_weight': 9.0,  \n",
    "    'min_split_gain': 0.09,  \n",
    "    'max_bin': 420,\n",
    "    'scale_pos_weight': 10,\n",
    "    'early_stopping_round': 100,\n",
    "    'n_jobs': -1,\n",
    "    }\n",
    "\n",
    "num_boost_round = 20000\n",
    "\n",
    "# Create LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "# Train model\n",
    "bst = lgb.train(\n",
    "    param,\n",
    "    train_data, \n",
    "    valid_sets=[valid_data],\n",
    "    num_boost_round=num_boost_round,\n",
    "    )\n",
    "    \n",
    "# Predict and evaluate\n",
    "y_val_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
    "auc = roc_auc_score(y_val, y_val_pred)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_proba = bst.predict(X_train, num_iteration=bst.best_iteration)\n",
    "y_val_pred_proba = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
    "\n",
    "# Calculate ROC AUC scores\n",
    "roc_auc_train = roc_auc_score(y_train, y_train_pred_proba)\n",
    "roc_auc_val = roc_auc_score(y_val, y_val_pred_proba)\n",
    "\n",
    "# Print ROC AUC scores\n",
    "print(f'Training ROC AUC Score: {roc_auc_train}')\n",
    "print(f'Validation ROC AUC Score: {roc_auc_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from datetime import datetime\n",
    "# import os\n",
    "\n",
    "# def objective(trial):\n",
    "#     param = {\n",
    "#         'objective': 'binary',\n",
    "#         'metric': 'auc',\n",
    "#         'lambda_l1': trial.suggest_float('reg_alpha', 0.01, 0.1, log=True),  \n",
    "#         'lambda_l2': trial.suggest_float('reg_lambda', 0.1, 0.3, log=True),  \n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.75, 0.85), \n",
    "#         'subsample': trial.suggest_float('subsample', 0.85, 1.0), \n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.1),  \n",
    "#         'max_depth': trial.suggest_int('max_depth', 16, 32), \n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 700, 1500), \n",
    "#         'min_child_samples': trial.suggest_int('min_child_samples', 20, 80),  \n",
    "#         'min_child_weight': trial.suggest_float('min_child_weight', 4.0, 10.0),  \n",
    "#         'min_split_gain': trial.suggest_float('min_split_gain', 0.05, 0.1),  \n",
    "#         'max_bin': trial.suggest_int('max_bin', 400, 500),\n",
    "#         'verbose': -1,  \n",
    "#         'scale_pos_weight': trial.suggest_float('scale_pos_weight', 9, 12),\n",
    "#         'early_stopping_round': 100,\n",
    "#     }\n",
    "\n",
    "#     num_boost_round = trial.suggest_int('num_boost_round', 10000, 20000)\n",
    "\n",
    "#     # Create LightGBM dataset\n",
    "#     train_data = lgb.Dataset(X_train, label=y_train)\n",
    "#     valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "#     # Train model\n",
    "#     bst = lgb.train(\n",
    "#         param,\n",
    "#         train_data, \n",
    "#         num_boost_round=num_boost_round,\n",
    "#         valid_sets=[valid_data],\n",
    "#     )\n",
    "    \n",
    "#     # Predict and evaluate\n",
    "#     y_val_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
    "#     auc = roc_auc_score(y_val, y_val_pred)\n",
    "#     return auc\n",
    "\n",
    "# # Generate a unique filename for each run\n",
    "# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# sqlite_file_path = os.path.join(r'C:\\Users\\paulo\\OneDrive\\Documents\\Binary-Classification-of-Insurance-Cross-Selling', f'optuna_study_{timestamp}.db')\n",
    "\n",
    "# # Create a study and optimize\n",
    "# study = optuna.create_study(storage=f'sqlite:///{sqlite_file_path}', study_name=f'my_study_{timestamp}', direction='maximize')\n",
    "# study.optimize(objective, n_trials=10)\n",
    "\n",
    "# # Get the best trial\n",
    "# best_trial = study.best_trial\n",
    "# print(f'Best trial score: {best_trial.value}')\n",
    "# print(f'Best trial params: {best_trial.params}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
