{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from scipy.optimize import minimize\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a log filename with the notebook name and current datetime\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_filename = f'kaggle_submission_{current_time}.log'\n",
    "\n",
    "# Configure logging to save to a file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()  # This ensures logs are also output to the console\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        logging.info(f'Start memory usage of dataframe: {start_mem:.2f} MB')\n",
    "\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        logging.info(f'End memory usage of dataframe: {end_mem:.2f} MB')\n",
    "        logging.info(f'Decreased by {(100 * (start_mem - end_mem) / start_mem):.1f}%')\n",
    "\n",
    "    return df\n",
    "\n",
    "def safe_map(df, column, mapping):\n",
    "    \"\"\"Map categorical values to numerical values and log any unknown categories.\"\"\"\n",
    "    unknown_categories = set(df[column]) - set(mapping.keys())\n",
    "    if unknown_categories:\n",
    "        logging.warning(f\"Unknown categories in column {column}: {unknown_categories}\")\n",
    "    df[column] = df[column].map(mapping)\n",
    "    return df\n",
    "\n",
    "def import_data(file, **kwargs):\n",
    "    \"\"\"Create a dataframe and optimize its memory usage.\"\"\"\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True, **kwargs)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess the dataset.\"\"\"\n",
    "    gender_mapping = {'Male': 1, 'Female': 0}\n",
    "    vehicle_damage_mapping = {'Yes': 1, 'No': 0}\n",
    "    vehicle_age_mapping = {'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2}\n",
    "    \n",
    "    df = safe_map(df, 'Gender', gender_mapping)\n",
    "    df = safe_map(df, 'Vehicle_Damage', vehicle_damage_mapping)\n",
    "    df = safe_map(df, 'Vehicle_Age', vehicle_age_mapping)\n",
    "    \n",
    "    # Check if 'Driving_License' column exists before dropping it\n",
    "    if 'Driving_License' in df.columns:\n",
    "        df.drop(['Driving_License'], axis=1, inplace=True)\n",
    "    else:\n",
    "        logging.warning(\"'Driving_License' column not found in the dataset.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"Feature engineering on the dataset.\"\"\"\n",
    "    df['Previously_Insured_Annual_Premium'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Annual_Premium'].astype(str)))[0]\n",
    "    df['Previously_Insured_Vehicle_Age'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Vehicle_Age'].astype(str)))[0]\n",
    "    df['Previously_Insured_Vehicle_Damage'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Vehicle_Damage'].astype(str)))[0]\n",
    "    df['Previously_Insured_Vintage'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Vintage'].astype(str)))[0]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 16:13:53,320 - INFO - Start memory usage of dataframe: 643.68 MB\n",
      "2024-07-25 16:13:54,393 - INFO - End memory usage of dataframe: 204.81 MB\n",
      "2024-07-25 16:13:54,394 - INFO - Decreased by 68.2%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_path = r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\test.csv\"\n",
    "\n",
    "test_df = import_data(test_path, index_col='id')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "test_df = preprocess_data(test_df)\n",
    "\n",
    "test_df = feature_engineering(test_df)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize numeric columns\n",
    "num_cols = ['Age', 'Region_Code', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']\n",
    "scaler = StandardScaler()\n",
    "test_df[num_cols] = scaler.fit_transform(test_df[num_cols])\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store predictions\n",
    "cat_preds = []\n",
    "lgb_preds = []\n",
    "xgb_preds = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a Pool object for CatBoost\n",
    "test_pool = Pool(test_df.astype(str), cat_features=test_df.columns.values)\n",
    "\n",
    "# Make predictions with CatBoost models\n",
    "for i in range(5):\n",
    "    model = joblib.load(f'catboost_model_fold_{i+1}.pkl')\n",
    "    test_pred = model.predict_proba(test_pool)[:, 1]\n",
    "    cat_preds.append(test_pred)\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "# Average the predictions from each fold for CatBoost\n",
    "test_pred_cat = np.mean(cat_preds, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions with LightGBM models\n",
    "for i in range(5):\n",
    "    model = joblib.load(f'lgb_model_fold_{i+1}.pkl')\n",
    "    test_pred = model.predict(test_df, num_iteration=model.best_iteration)\n",
    "    lgb_preds.append(test_pred)\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "# Average the predictions from each fold for LightGBM\n",
    "test_pred_lgb = np.mean(lgb_preds, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions with XGBoost models\n",
    "dtest = xgb.DMatrix(test_df, enable_categorical=True)\n",
    "for i in range(5):\n",
    "    model = xgb.Booster()\n",
    "    model.load_model(f'xgb_model_fold_{i+1}.json')\n",
    "    test_pred = model.predict(dtest, iteration_range=(0, model.best_iteration))\n",
    "    xgb_preds.append(test_pred)\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "# Average the predictions from each fold for XGBoost\n",
    "test_pred_xgb = np.mean(xgb_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission files created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Get current time for filenames\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Create submission DataFrame for CatBoost\n",
    "submission_cat = pd.DataFrame({\n",
    "    'id': test_df.index,\n",
    "    'Response': test_pred_cat\n",
    "})\n",
    "submission_cat.to_csv(f'submission_cat_{current_time}.csv', index=False)\n",
    "\n",
    "# Create submission DataFrame for LightGBM\n",
    "submission_lgb = pd.DataFrame({\n",
    "    'id': test_df.index,\n",
    "    'Response': test_pred_lgb\n",
    "})\n",
    "submission_lgb.to_csv(f'submission_lgb_{current_time}.csv', index=False)\n",
    "\n",
    "# Create submission DataFrame for XGBoost\n",
    "submission_xgb = pd.DataFrame({\n",
    "    'id': test_df.index,\n",
    "    'Response': test_pred_xgb\n",
    "})\n",
    "submission_xgb.to_csv(f'submission_xgb_{current_time}.csv', index=False)\n",
    "\n",
    "print(\"Submission files created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blended submission file (LightGBM + XGBoost) created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Blend LightGBM and XGBoost predictions\n",
    "blend_weight = 0.5  # You can adjust this weight based on validation performance\n",
    "test_pred_blend = (blend_weight * test_pred_lgb) + ((1 - blend_weight) * test_pred_xgb)\n",
    "\n",
    "# Create submission DataFrame for the blended predictions\n",
    "submission_blend = pd.DataFrame({\n",
    "    'id': test_df.index,\n",
    "    'Response': test_pred_blend\n",
    "})\n",
    "submission_blend.to_csv(f'submission_blend_lgb_xgb_{current_time}.csv', index=False)\n",
    "\n",
    "print(\"Blended submission file (LightGBM + XGBoost) created successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
