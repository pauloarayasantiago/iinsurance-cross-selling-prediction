{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T13:24:32.747480Z","iopub.status.busy":"2024-07-26T13:24:32.747112Z","iopub.status.idle":"2024-07-26T13:24:48.779491Z","shell.execute_reply":"2024-07-26T13:24:48.778005Z","shell.execute_reply.started":"2024-07-26T13:24:32.747446Z"},"trusted":true},"outputs":[],"source":["%%capture\n","!pip install klib"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T13:24:48.782646Z","iopub.status.busy":"2024-07-26T13:24:48.782116Z","iopub.status.idle":"2024-07-26T13:24:53.419493Z","shell.execute_reply":"2024-07-26T13:24:53.418467Z","shell.execute_reply.started":"2024-07-26T13:24:48.782583Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import logging\n","import gc\n","import klib\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import roc_auc_score\n","import lightgbm as lgb\n","import xgboost as xgb\n","from catboost import CatBoostClassifier, Pool\n","from datetime import datetime\n","import warnings\n","import joblib\n","import seaborn as sns\n","\n","\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T13:24:53.425698Z","iopub.status.busy":"2024-07-26T13:24:53.425338Z","iopub.status.idle":"2024-07-26T13:24:53.432135Z","shell.execute_reply":"2024-07-26T13:24:53.431069Z","shell.execute_reply.started":"2024-07-26T13:24:53.425656Z"},"trusted":true},"outputs":[],"source":["# Function to create a log filename with the notebook name and current datetime\n","current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","log_filename = f'kaggle_submission_{current_time}.log'\n","\n","# Configure logging to save to a file and output to the console\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    handlers=[\n","        logging.FileHandler(log_filename),\n","        logging.StreamHandler()  # This ensures logs are also output to the console\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T13:24:53.433838Z","iopub.status.busy":"2024-07-26T13:24:53.433497Z","iopub.status.idle":"2024-07-26T13:25:17.176656Z","shell.execute_reply":"2024-07-26T13:25:17.175762Z","shell.execute_reply.started":"2024-07-26T13:24:53.433810Z"},"trusted":true},"outputs":[],"source":["# Paths to datasets\n","train_path = r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\train.csv\"\n","test_path = r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\test.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T13:25:26.078078Z","iopub.status.busy":"2024-07-26T13:25:26.077681Z","iopub.status.idle":"2024-07-26T13:25:26.113813Z","shell.execute_reply":"2024-07-26T13:25:26.112507Z","shell.execute_reply.started":"2024-07-26T13:25:26.078040Z"},"trusted":true},"outputs":[],"source":["# Documenting the purpose and usage of the function\n","def get_column_stats(df):\n","    \"\"\"Get basic statistics for each column in the dataframe.\"\"\"\n","    stats = {}\n","    for col in df.columns:\n","        if pd.api.types.is_numeric_dtype(df[col]):\n","            stats[col] = {\n","                'min': df[col].min(),\n","                'max': df[col].max(),\n","                'mean': df[col].mean(),\n","            }\n","        else:\n","            stats[col] = {\n","                'unique': df[col].nunique()\n","            }\n","    return stats\n","\n","# Log comparison of statistics\n","def compare_stats(stats_before, stats_after):\n","    \"\"\"Compare statistics before and after type conversion.\"\"\"\n","    for col in stats_before:\n","        if stats_before[col] != stats_after[col]:\n","            logging.warning(f\"Column {col} has changed: {stats_before[col]} != {stats_after[col]}\")\n","\n","# Log precision loss\n","def calculate_precision_loss(stats_before, stats_after):\n","    \"\"\"Calculate and log precision loss for numeric columns.\"\"\"\n","    for col in stats_before:\n","        if 'mean' in stats_before[col]:\n","            mean_before = stats_before[col]['mean']\n","            mean_after = stats_after[col]['mean']\n","            precision_loss = abs(mean_before - mean_after) / abs(mean_before) * 100\n","            logging.info(f\"Column {col} precision loss: {precision_loss:.6f}%\")\n","\n","# Memory optimization function\n","def reduce_mem_usage(df, verbose=True):\n","    \"\"\"Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\"\"\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    if verbose:\n","        logging.info(f'Start memory usage of dataframe: {start_mem:.2f} MB')\n","\n","    for col in df.columns:\n","        col_type = df[col].dtype\n","        \n","        if col_type != object and col_type.name != 'category':\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","        elif col_type == object:\n","            df[col] = df[col].astype('category')\n","\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    if verbose:\n","        logging.info(f'End memory usage of dataframe: {end_mem:.2f} MB')\n","        logging.info(f'Decreased by {(100 * (start_mem - end_mem) / start_mem):.1f}%')\n","\n","    return df\n","\n","# Function to import data with logging\n","def import_data(path, index_col=None):\n","    \"\"\"Import data from a CSV file and optimize memory usage.\"\"\"\n","    try:\n","        df = pd.read_csv(path, index_col=index_col)\n","        \n","        # Get column stats before optimization\n","        stats_before = get_column_stats(df)\n","        \n","        df = reduce_mem_usage(df)\n","        \n","        # Get column stats after optimization\n","        stats_after = get_column_stats(df)\n","        \n","        # Compare statistics and calculate precision loss\n","        compare_stats(stats_before, stats_after)\n","        calculate_precision_loss(stats_before, stats_after)\n","        \n","        logging.info(f'Data loaded and memory optimized from {path}')\n","        return df\n","    except Exception as e:\n","        logging.error(f'Error loading data from {path}: {str(e)}')\n","        return None\n","\n","# Log any unknown categories during mapping\n","def safe_map(df, column, mapping):\n","    \"\"\"Map categorical values to numerical values and log any unknown categories.\"\"\"\n","    unknown_categories = set(df[column]) - set(mapping.keys())\n","    if unknown_categories:\n","        logging.warning(f\"Unknown categories in column {column}: {unknown_categories}\")\n","    df[column] = df[column].map(mapping)\n","    return df\n","\n","# Preprocess data with logging\n","def preprocess_data(df):\n","    \"\"\"Preprocess the dataset.\"\"\"\n","    gender_mapping = {'Male': 1, 'Female': 0}\n","    vehicle_damage_mapping = {'Yes': 1, 'No': 0}\n","    vehicle_age_mapping = {'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2}\n","    \n","    df = safe_map(df, 'Gender', gender_mapping)\n","    df = safe_map(df, 'Vehicle_Damage', vehicle_damage_mapping)\n","    df = safe_map(df, 'Vehicle_Age', vehicle_age_mapping)\n","    \n","    df.drop(['Driving_License'], axis=1, inplace=True)\n","    logging.info(\"Data preprocessing completed.\")\n","    return df\n","\n","# Feature engineering function with logging\n","def feature_engineering(df):\n","    \"\"\"Feature engineering on the dataset.\"\"\"\n","    df['Previously_Insured_Annual_Premium'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Annual_Premium'].astype(str)))[0]\n","    df['Previously_Insured_Vehicle_Age'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Vehicle_Age'].astype(str)))[0]\n","    df['Previously_Insured_Vehicle_Damage'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Vehicle_Damage'].astype(str)))[0]\n","    df['Previously_Insured_Vintage'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Vintage'].astype(str)))[0]\n","    logging.info(\"Feature engineering completed.\")\n","    return df\n","\n","def reduce_mem_usage_for_engineered_features(df):\n","    df['Previously_Insured_Annual_Premium'] = reduce_mem_usage(df[['Previously_Insured_Annual_Premium']])\n","    df['Previously_Insured_Vehicle_Age'] = reduce_mem_usage(df[['Previously_Insured_Vehicle_Age']])\n","    df['Previously_Insured_Vehicle_Damage'] = reduce_mem_usage(df[['Previously_Insured_Vehicle_Damage']])\n","    df['Previously_Insured_Vintage'] = reduce_mem_usage(df[['Previously_Insured_Vintage']])\n","    return df\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T13:25:26.115842Z","iopub.status.busy":"2024-07-26T13:25:26.115406Z","iopub.status.idle":"2024-07-26T13:26:11.798470Z","shell.execute_reply":"2024-07-26T13:26:11.797538Z","shell.execute_reply.started":"2024-07-26T13:25:26.115801Z"},"trusted":true},"outputs":[],"source":["# Load and optimize data\n","train_df = import_data(train_path, index_col='id')\n","test_df = import_data(test_path, index_col='id')\n","\n","gc.collect()\n","logging.info(\"Data loaded successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T13:26:11.800567Z","iopub.status.busy":"2024-07-26T13:26:11.800160Z","iopub.status.idle":"2024-07-26T13:26:13.235176Z","shell.execute_reply":"2024-07-26T13:26:13.234191Z","shell.execute_reply.started":"2024-07-26T13:26:11.800529Z"},"trusted":true},"outputs":[],"source":["# Apply preprocessing\n","train_df = preprocess_data(train_df)\n","test_df = preprocess_data(test_df)\n","logging.info(\"Data preprocessed successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T13:26:13.236909Z","iopub.status.busy":"2024-07-26T13:26:13.236515Z","iopub.status.idle":"2024-07-26T13:26:51.666685Z","shell.execute_reply":"2024-07-26T13:26:51.665824Z","shell.execute_reply.started":"2024-07-26T13:26:13.236871Z"},"trusted":true},"outputs":[],"source":["# Apply feature engineering\n","train_df = feature_engineering(train_df)\n","test_df = feature_engineering(test_df)\n","\n","gc.collect()\n","logging.info(\"Feature engineering completed successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Apply memory optimization to the engineered features\n","train_df = reduce_mem_usage_for_engineered_features(train_df)\n","test_df = reduce_mem_usage_for_engineered_features(test_df)\n","logging.info(\"Engineered features memory optimization completed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T13:26:51.668106Z","iopub.status.busy":"2024-07-26T13:26:51.667808Z","iopub.status.idle":"2024-07-26T13:26:52.078564Z","shell.execute_reply":"2024-07-26T13:26:52.077321Z","shell.execute_reply.started":"2024-07-26T13:26:51.668079Z"},"trusted":true},"outputs":[],"source":["# Normalize numeric columns\n","num_cols = ['Age', 'Region_Code', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']\n","scaler = StandardScaler()\n","train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n","test_df[num_cols] = scaler.transform(test_df[num_cols])\n","logging.info(\"Numeric columns normalized.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T13:26:52.080762Z","iopub.status.busy":"2024-07-26T13:26:52.080201Z","iopub.status.idle":"2024-07-26T13:26:52.094309Z","shell.execute_reply":"2024-07-26T13:26:52.092700Z","shell.execute_reply.started":"2024-07-26T13:26:52.080722Z"},"trusted":true},"outputs":[],"source":["# Separate features and target variable\n","X = train_df.drop('Response', axis=1)\n","y = train_df['Response']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create Stratified K-Folds\n","n_splits = 5\n","skfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n","categorical_features = ['Gender', 'Vehicle_Age', 'Vehicle_Damage', 'Previously_Insured_Annual_Premium', 'Previously_Insured_Vehicle_Age', 'Previously_Insured_Vehicle_Damage', 'Previously_Insured_Vintage']\n","\n","# XGBoost\n","dtrain = xgb.DMatrix(X, label=y, enable_categorical=True)\n","dtest = xgb.DMatrix(test_df, enable_categorical=True)\n","\n","# LightGBM\n","train_data = lgb.Dataset(X, label=y, categorical_feature=categorical_features)\n","\n","# CatBoost\n","train_pool = Pool(X, y, cat_features=categorical_features)\n","test_pool = Pool(test_df, cat_features=categorical_features)\n","\n","logging.info(\"Data prepared for modeling.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T13:26:52.104420Z","iopub.status.busy":"2024-07-26T13:26:52.103741Z","iopub.status.idle":"2024-07-26T13:26:52.115421Z","shell.execute_reply":"2024-07-26T13:26:52.111374Z","shell.execute_reply.started":"2024-07-26T13:26:52.104382Z"},"trusted":true},"outputs":[],"source":["import optuna\n","from optuna.samplers import TPESampler\n","from catboost import CatBoostClassifier, Pool\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score\n","import json\n","import gc\n","\n","# Split the data into training and validation sets\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n","\n","# Objective function for Optuna\n","def objective(trial):\n","    params = {\n","        'loss_function': 'Logloss',\n","        'eval_metric': 'AUC',\n","        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n","        'iterations': trial.suggest_int('iterations', 1000, 3000),\n","        'depth': trial.suggest_int('depth', 4, 10),\n","        'random_strength': trial.suggest_uniform('random_strength', 0, 10),\n","        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-2, 10),\n","        'bagging_temperature': trial.suggest_uniform('bagging_temperature', 0, 1),\n","        'border_count': trial.suggest_int('border_count', 1, 255),\n","        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n","        'max_bin': trial.suggest_int('max_bin', 200, 300),\n","        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n","        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n","        'task_type': 'GPU',\n","        'verbose': 100\n","    }\n","    \n","    train_pool = Pool(X_train, y_train, cat_features=categorical_features)\n","    valid_pool = Pool(X_valid, y_valid, cat_features=categorical_features)\n","    \n","    model = CatBoostClassifier(**params)\n","    model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=50, verbose=100)\n","    \n","    valid_preds = model.predict_proba(X_valid)[:, 1]\n","    auc_score = roc_auc_score(y_valid, valid_preds)\n","    \n","    # Clear memory\n","    del model, train_pool, valid_pool, valid_preds\n","    gc.collect()\n","    \n","    return auc_score\n","\n","# Create study and optimize\n","study = optuna.create_study(direction='maximize', sampler=TPESampler())\n","study.optimize(objective, n_trials=50)\n","\n","# Log best parameters\n","best_params = study.best_params\n","logging.info(f\"Best parameters: {best_params}\")\n","\n","# Save the best parameters for later use\n","with open(\"best_catboost_params.json\", \"w\") as f:\n","    json.dump(best_params, f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T13:26:52.118020Z","iopub.status.busy":"2024-07-26T13:26:52.117554Z","iopub.status.idle":"2024-07-26T13:26:52.131124Z","shell.execute_reply":"2024-07-26T13:26:52.126682Z","shell.execute_reply.started":"2024-07-26T13:26:52.117933Z"},"trusted":true},"outputs":[],"source":["import optuna\n","import lightgbm as lgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score\n","import json\n","import gc\n","\n","# Split the data into training and validation sets\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n","\n","# Objective function for Optuna\n","def objective(trial):\n","    params = {\n","        'objective': 'binary',\n","        'metric': 'auc',\n","        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n","        'num_leaves': trial.suggest_int('num_leaves', 31, 256),\n","        'max_depth': trial.suggest_int('max_depth', 4, 16),\n","        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n","        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n","        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n","        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n","        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n","        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n","        'min_gain_to_split': trial.suggest_loguniform('min_gain_to_split', 1e-8, 10.0),\n","        'max_bin': trial.suggest_int('max_bin', 200, 300),\n","        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart', 'goss']),\n","        'device': 'gpu',\n","        'verbosity': -1,\n","        'early_stopping_round': 50\n","    }\n","    \n","    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n","    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data, categorical_feature=categorical_features)\n","    \n","    model = lgb.train(params, train_data, valid_sets=[train_data, valid_data], early_stopping_rounds=50, verbose_eval=100)\n","    \n","    valid_preds = model.predict(X_valid, num_iteration=model.best_iteration)\n","    auc_score = roc_auc_score(y_valid, valid_preds)\n","    \n","    # Clear memory\n","    del model, train_data, valid_data, valid_preds\n","    gc.collect()\n","    \n","    return auc_score\n","\n","# Create study and optimize\n","study = optuna.create_study(direction='maximize', sampler=TPESampler())\n","study.optimize(objective, n_trials=50)\n","\n","# Log best parameters\n","best_params = study.best_params\n","logging.info(f\"Best parameters: {best_params}\")\n","\n","# Save the best parameters for later use\n","with open(\"best_lightgbm_params.json\", \"w\") as f:\n","    json.dump(best_params, f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T13:26:52.134706Z","iopub.status.busy":"2024-07-26T13:26:52.134294Z","iopub.status.idle":"2024-07-26T13:26:52.142959Z","shell.execute_reply":"2024-07-26T13:26:52.141760Z","shell.execute_reply.started":"2024-07-26T13:26:52.134667Z"},"trusted":true},"outputs":[],"source":["import optuna\n","import xgboost as xgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score\n","import json\n","import gc\n","\n","# Split the data into training and validation sets\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n","\n","# Objective function for Optuna\n","def objective(trial):\n","    params = {\n","        'eval_metric': 'auc',\n","        'eta': trial.suggest_loguniform('eta', 0.01, 0.3),\n","        'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n","        'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n","        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n","        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n","        'max_depth': trial.suggest_int('max_depth', 4, 16),\n","        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n","        'gamma': trial.suggest_loguniform('gamma', 1e-8, 10.0),\n","        'max_bin': trial.suggest_int('max_bin', 200, 300),\n","        'tree_method': 'gpu_hist',\n","        'predictor': 'gpu_predictor',\n","        'enable_categorical': True,\n","        'verbosity': 1\n","    }\n","    \n","    dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n","    dvalid = xgb.DMatrix(X_valid, label=y_valid, enable_categorical=True)\n","    \n","    model = xgb.train(\n","        params,\n","        dtrain,\n","        num_boost_round=3000,\n","        evals=[(dtrain, 'train'), (dvalid, 'valid')],\n","        early_stopping_rounds=50,\n","        verbose_eval=100\n","    )\n","    \n","    valid_preds = model.predict(dvalid, iteration_range=(0, model.best_iteration))\n","    auc_score = roc_auc_score(y_valid, valid_preds)\n","    \n","    # Clear memory\n","    del model, dtrain, dvalid, valid_preds\n","    gc.collect()\n","    \n","    return auc_score\n","\n","# Create study and optimize\n","study = optuna.create_study(direction='maximize', sampler=TPESampler())\n","study.optimize(objective, n_trials=50)\n","\n","# Log best parameters\n","best_params = study.best_params\n","logging.info(f\"Best parameters: {best_params}\")\n","\n","# Save the best parameters for later use\n","with open(\"best_xgboost_params.json\", \"w\") as f:\n","    json.dump(best_params, f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T13:26:52.144975Z","iopub.status.busy":"2024-07-26T13:26:52.144493Z","iopub.status.idle":"2024-07-26T13:27:30.642210Z","shell.execute_reply":"2024-07-26T13:27:30.641278Z","shell.execute_reply.started":"2024-07-26T13:26:52.144933Z"},"trusted":true},"outputs":[],"source":["# # Load the prediction files\n","# test_pred_cat = pd.read_csv('/kaggle/input/blend-lgbm-xgb-catboost-cat/submission.csv')\n","# test_pred_lgb = pd.read_csv('/kaggle/input/blend-lgbm-xgb-catboost-xgb/submission.csv')\n","# test_pred_xgb = pd.read_csv('/kaggle/input/blend-lgbm-xgb-catboost-lgbm/submission.csv')\n","\n","# # Ensure we are using the correct columns for blending\n","# blended_preds = (test_pred_cat['Response'] + test_pred_lgb['Response'] + test_pred_xgb['Response']) / 3\n","\n","# # Create the submission DataFrame\n","# submission = pd.DataFrame({\n","#     'id': test_pred_cat['id'],  # Assuming 'id' column is the same in all files\n","#     'Response': blended_preds\n","# })\n","\n","# # Save the submission file\n","# submission.to_csv(\"submission.csv\", index=False)\n","\n","# print(\"Submission file created successfully!\")\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8930475,"sourceId":73291,"sourceType":"competition"},{"sourceId":189806873,"sourceType":"kernelVersion"},{"sourceId":189806878,"sourceType":"kernelVersion"},{"sourceId":189813334,"sourceType":"kernelVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
