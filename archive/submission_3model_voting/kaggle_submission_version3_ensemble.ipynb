{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from scipy.optimize import minimize\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a log filename with the notebook name and current datetime\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_filename = f'kaggle_submission_{current_time}.log'\n",
    "\n",
    "# Configure logging to save to a file and output to the console\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()  # This ensures logs are also output to the console\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documenting the purpose and usage of the function\n",
    "def get_column_stats(df):\n",
    "    \"\"\"Get basic statistics for each column in the dataframe.\"\"\"\n",
    "    stats = {}\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            stats[col] = {\n",
    "                'min': df[col].min(),\n",
    "                'max': df[col].max(),\n",
    "                'mean': df[col].mean(),\n",
    "            }\n",
    "        else:\n",
    "            stats[col] = {\n",
    "                'unique': df[col].nunique()\n",
    "            }\n",
    "    return stats\n",
    "\n",
    "# Log comparison of statistics\n",
    "def compare_stats(stats_before, stats_after):\n",
    "    \"\"\"Compare statistics before and after type conversion.\"\"\"\n",
    "    for col in stats_before:\n",
    "        if stats_before[col] != stats_after[col]:\n",
    "            logging.warning(f\"Column {col} has changed: {stats_before[col]} != {stats_after[col]}\")\n",
    "\n",
    "# Log precision loss\n",
    "def calculate_precision_loss(stats_before, stats_after):\n",
    "    \"\"\"Calculate and log precision loss for numeric columns.\"\"\"\n",
    "    for col in stats_before:\n",
    "        if 'mean' in stats_before[col]:\n",
    "            mean_before = stats_before[col]['mean']\n",
    "            mean_after = stats_after[col]['mean']\n",
    "            precision_loss = abs(mean_before - mean_after) / abs(mean_before) * 100\n",
    "            logging.info(f\"Column {col} precision loss: {precision_loss:.6f}%\")\n",
    "\n",
    "# Memory optimization function\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        logging.info(f'Start memory usage of dataframe: {start_mem:.2f} MB')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        logging.info(f'End memory usage of dataframe: {end_mem:.2f} MB')\n",
    "        logging.info(f'Decreased by {(100 * (start_mem - end_mem) / start_mem):.1f}%')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Log any unknown categories during mapping\n",
    "def safe_map(df, column, mapping):\n",
    "    \"\"\"Map categorical values to numerical values and log any unknown categories.\"\"\"\n",
    "    unknown_categories = set(df[column]) - set(mapping.keys())\n",
    "    if unknown_categories:\n",
    "        logging.warning(f\"Unknown categories in column {column}: {unknown_categories}\")\n",
    "    df[column] = df[column].map(mapping)\n",
    "    return df\n",
    "\n",
    "# Function to import data with logging\n",
    "def import_data(path, index_col=None):\n",
    "    \"\"\"Import data from a CSV file and optimize memory usage.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, index_col=index_col)\n",
    "        \n",
    "        # Get column stats before optimization\n",
    "        stats_before = get_column_stats(df)\n",
    "        \n",
    "        df = reduce_mem_usage(df)\n",
    "        \n",
    "        # Get column stats after optimization\n",
    "        stats_after = get_column_stats(df)\n",
    "        \n",
    "        # Compare statistics and calculate precision loss\n",
    "        compare_stats(stats_before, stats_after)\n",
    "        calculate_precision_loss(stats_before, stats_after)\n",
    "        \n",
    "        logging.info(f'Data loaded and memory optimized from {path}')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error loading data from {path}: {str(e)}')\n",
    "        return None\n",
    "\n",
    "# Preprocess data with logging\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess the dataset.\"\"\"\n",
    "    gender_mapping = {'Male': 1, 'Female': 0}\n",
    "    vehicle_damage_mapping = {'Yes': 1, 'No': 0}\n",
    "    vehicle_age_mapping = {'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2}\n",
    "    \n",
    "    df = safe_map(df, 'Gender', gender_mapping)\n",
    "    df = safe_map(df, 'Vehicle_Damage', vehicle_damage_mapping)\n",
    "    df = safe_map(df, 'Vehicle_Age', vehicle_age_mapping)\n",
    "    \n",
    "    df.drop(['Driving_License'], axis=1, inplace=True)\n",
    "    logging.info(\"Data preprocessing completed.\")\n",
    "    return df\n",
    "\n",
    "# Feature engineering function with logging\n",
    "def feature_engineering(df):\n",
    "    \"\"\"Feature engineering on the dataset.\"\"\"\n",
    "    df['Previously_Insured_Annual_Premium'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Annual_Premium'].astype(str)))[0]\n",
    "    df['Previously_Insured_Vehicle_Age'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Vehicle_Age'].astype(str)))[0]\n",
    "    df['Previously_Insured_Vehicle_Damage'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Vehicle_Damage'].astype(str)))[0]\n",
    "    df['Previously_Insured_Vintage'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Vintage'].astype(str)))[0]\n",
    "    logging.info(\"Feature engineering completed.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:33:13,062 - INFO - Start memory usage of dataframe: 1053.30 MB\n",
      "2024-07-24 16:33:14,761 - INFO - End memory usage of dataframe: 318.18 MB\n",
      "2024-07-24 16:33:14,763 - INFO - Decreased by 69.8%\n",
      "2024-07-24 16:33:15,111 - WARNING - Column Region_Code has changed: {'min': 0.0, 'max': 52.0, 'mean': 26.41868976752134} != {'min': 0.0, 'max': 52.0, 'mean': 26.418718}\n",
      "2024-07-24 16:33:15,112 - WARNING - Column Annual_Premium has changed: {'min': 2630.0, 'max': 540165.0, 'mean': 30461.370410588694} != {'min': 2630.0, 'max': 540165.0, 'mean': 30461.36}\n",
      "2024-07-24 16:33:15,112 - WARNING - Column Policy_Sales_Channel has changed: {'min': 1.0, 'max': 163.0, 'mean': 112.42544188954903} != {'min': 1.0, 'max': 163.0, 'mean': 112.42544}\n",
      "2024-07-24 16:33:15,113 - INFO - Column Age precision loss: 0.000000%\n",
      "2024-07-24 16:33:15,114 - INFO - Column Driving_License precision loss: 0.000000%\n",
      "2024-07-24 16:33:15,114 - INFO - Column Region_Code precision loss: 0.000108%\n",
      "2024-07-24 16:33:15,115 - INFO - Column Previously_Insured precision loss: 0.000000%\n",
      "2024-07-24 16:33:15,115 - INFO - Column Annual_Premium precision loss: 0.000036%\n",
      "2024-07-24 16:33:15,115 - INFO - Column Policy_Sales_Channel precision loss: 0.000004%\n",
      "2024-07-24 16:33:15,115 - INFO - Column Vintage precision loss: 0.000000%\n",
      "2024-07-24 16:33:15,117 - INFO - Column Response precision loss: 0.000000%\n",
      "2024-07-24 16:33:15,117 - INFO - Data loaded and memory optimized from C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\train.csv\n",
      "2024-07-24 16:33:20,529 - INFO - Start memory usage of dataframe: 643.68 MB\n",
      "2024-07-24 16:33:21,615 - INFO - End memory usage of dataframe: 204.81 MB\n",
      "2024-07-24 16:33:21,616 - INFO - Decreased by 68.2%\n",
      "2024-07-24 16:33:21,851 - WARNING - Column Region_Code has changed: {'min': 0.0, 'max': 52.0, 'mean': 26.42661358099346} != {'min': 0.0, 'max': 52.0, 'mean': 26.426613}\n",
      "2024-07-24 16:33:21,852 - WARNING - Column Annual_Premium has changed: {'min': 2630.0, 'max': 540165.0, 'mean': 30465.52566837543} != {'min': 2630.0, 'max': 540165.0, 'mean': 30465.523}\n",
      "2024-07-24 16:33:21,852 - WARNING - Column Policy_Sales_Channel has changed: {'min': 1.0, 'max': 163.0, 'mean': 112.36499203506294} != {'min': 1.0, 'max': 163.0, 'mean': 112.36501}\n",
      "2024-07-24 16:33:21,853 - INFO - Column Age precision loss: 0.000000%\n",
      "2024-07-24 16:33:21,853 - INFO - Column Driving_License precision loss: 0.000000%\n",
      "2024-07-24 16:33:21,854 - INFO - Column Region_Code precision loss: 0.000003%\n",
      "2024-07-24 16:33:21,854 - INFO - Column Previously_Insured precision loss: 0.000000%\n",
      "2024-07-24 16:33:21,854 - INFO - Column Annual_Premium precision loss: 0.000007%\n",
      "2024-07-24 16:33:21,855 - INFO - Column Policy_Sales_Channel precision loss: 0.000019%\n",
      "2024-07-24 16:33:21,856 - INFO - Column Vintage precision loss: 0.000000%\n",
      "2024-07-24 16:33:21,856 - INFO - Data loaded and memory optimized from C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\test.csv\n",
      "2024-07-24 16:33:21,917 - INFO - Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Paths to datasets\n",
    "train_path = r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\train.csv\"\n",
    "test_path = r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\test.csv\"\n",
    "\n",
    "# Load and optimize data\n",
    "train_df = import_data(train_path, index_col='id')\n",
    "test_df = import_data(test_path, index_col='id')\n",
    "\n",
    "gc.collect()\n",
    "logging.info(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:33:23,048 - INFO - Data preprocessing completed.\n",
      "2024-07-24 16:33:23,791 - INFO - Data preprocessing completed.\n",
      "2024-07-24 16:33:23,791 - INFO - Data preprocessed successfully.\n",
      "2024-07-24 16:33:44,916 - INFO - Feature engineering completed.\n",
      "2024-07-24 16:33:58,952 - INFO - Feature engineering completed.\n",
      "2024-07-24 16:33:59,009 - INFO - Feature engineering completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing\n",
    "train_df = preprocess_data(train_df)\n",
    "test_df = preprocess_data(test_df)\n",
    "logging.info(\"Data preprocessed successfully.\")\n",
    "\n",
    "# Apply feature engineering\n",
    "train_df = feature_engineering(train_df)\n",
    "test_df = feature_engineering(test_df)\n",
    "\n",
    "gc.collect()\n",
    "logging.info(\"Feature engineering completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:34:00,191 - INFO - Numeric columns normalized.\n"
     ]
    }
   ],
   "source": [
    "# Normalize numeric columns\n",
    "num_cols = ['Age', 'Region_Code', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']\n",
    "scaler = StandardScaler()\n",
    "train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n",
    "test_df[num_cols] = scaler.transform(test_df[num_cols])\n",
    "logging.info(\"Numeric columns normalized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = train_df.drop('Response', axis=1)\n",
    "y = train_df['Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Stratified K-Folds\n",
    "n_splits = 5\n",
    "skfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:34:55,475 - INFO - Starting CatBoost training...\n",
      "2024-07-24 16:34:56,645 - INFO - ---- CatBoost Fold 1 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.8696392\tbest: 0.8696392 (0)\ttotal: 12.6s\tremaining: 10h 29m 12s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m valid_pool \u001b[38;5;241m=\u001b[39m Pool(X_valid\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m), y_valid, cat_features\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     37\u001b[0m model \u001b[38;5;241m=\u001b[39m CatBoostClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcat_params)\n\u001b[1;32m---> 38\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m valid_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_valid\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m))[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     41\u001b[0m auc_score \u001b[38;5;241m=\u001b[39m roc_auc_score(y_valid, valid_preds)\n",
      "File \u001b[1;32mc:\\Users\\paulo\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\catboost\\core.py:5201\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5199\u001b[0m     CatBoostClassifier\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5201\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5202\u001b[0m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5203\u001b[0m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\paulo\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\catboost\\core.py:2396\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2393\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2395\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2396\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_sets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2404\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2405\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32mc:\\Users\\paulo\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\catboost\\core.py:1776\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1776\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1777\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:4833\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:4882\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define CatBoost parameters\n",
    "cat_params = {\n",
    "    'loss_function': 'Logloss',\n",
    "    'eval_metric': 'AUC',\n",
    "    'class_names': [0, 1],\n",
    "    'learning_rate': 0.075,\n",
    "    'iterations': 3000,\n",
    "    'depth': 9,\n",
    "    'random_strength': 0,\n",
    "    'l2_leaf_reg': 0.5,\n",
    "    'max_leaves': 512,\n",
    "    'fold_permutation_block': 64,\n",
    "    # 'task_type': 'GPU',  # Ensure your environment supports GPU\n",
    "    'random_seed': 42,\n",
    "    'allow_writing_files': False,\n",
    "    'verbose': 100,  # Display log every 100 iterations\n",
    "    # 'thread_count': -1\n",
    "}\n",
    "\n",
    "# Initialize lists to store out-of-fold predictions, models, and AUC scores\n",
    "cat_preds = []\n",
    "cat_aucs = []\n",
    "\n",
    "test_pool = Pool(test_df.astype(str), cat_features=X.columns.values)\n",
    "\n",
    "# CatBoost Model\n",
    "logging.info(\"Starting CatBoost training...\")\n",
    "for fold, (train_idx, test_idx) in enumerate(skfold.split(X, y)):\n",
    "    logging.info(f\"---- CatBoost Fold {fold + 1} ----\")\n",
    "    \n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_valid, y_valid = X.iloc[test_idx], y.iloc[test_idx]\n",
    "    \n",
    "    train_pool = Pool(X_train.astype(str), y_train, cat_features=X.columns.values)\n",
    "    valid_pool = Pool(X_valid.astype(str), y_valid, cat_features=X.columns.values)\n",
    "    \n",
    "    model = CatBoostClassifier(**cat_params)\n",
    "    model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=50, verbose=100)\n",
    "    \n",
    "    valid_preds = model.predict_proba(X_valid.astype(str))[:, 1]\n",
    "    auc_score = roc_auc_score(y_valid, valid_preds)\n",
    "    cat_aucs.append(auc_score)\n",
    "    logging.info(f\"Validation AUC score for fold {fold + 1}: {auc_score:.6f}\")\n",
    "    \n",
    "    test_pred = model.predict_proba(test_pool)[:, 1]\n",
    "    cat_preds.append(test_pred)\n",
    "    \n",
    "    # Save the model for this fold\n",
    "    joblib.dump(model, f'catboost_model_fold_{fold + 1}.pkl')\n",
    "    \n",
    "    # Clear memory\n",
    "    del X_train, y_train, X_valid, y_valid, train_pool, valid_pool, model\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# Calculate overall AUC score for CatBoost\n",
    "auc_mean_cat = np.mean(cat_aucs)\n",
    "auc_std_cat = np.std(cat_aucs)\n",
    "logging.info(f\"Overall ROC-AUC Score for CatBoost: {auc_mean_cat:.6f} ± {auc_std_cat:.6f}\")\n",
    "\n",
    "# Average the predictions from each fold for CatBoost\n",
    "test_pred_cat = np.mean(cat_preds, axis=0)\n",
    "joblib.dump(test_pred_cat, 'test_pred_cat.pkl')\n",
    "logging.info(\"CatBoost models and predictions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:29:58,129 - INFO - Starting LightGBM training...\n",
      "2024-07-24 16:29:58,136 - INFO - ---- Fold 1 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5642, number of negative: 40377\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1245\n",
      "[LightGBM] [Info] Number of data points in the train set: 46019, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122602 -> initscore=-1.968022\n",
      "[LightGBM] [Info] Start training from score -1.968022\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:29:59,872 - INFO - Validation AUC score for fold 1: 0.867926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[225]\ttraining's auc: 0.905537\tvalid_1's auc: 0.867926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:30:00,033 - INFO - ---- Fold 2 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5642, number of negative: 40377\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1242\n",
      "[LightGBM] [Info] Number of data points in the train set: 46019, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122602 -> initscore=-1.968022\n",
      "[LightGBM] [Info] Start training from score -1.968022\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:30:01,293 - INFO - Validation AUC score for fold 2: 0.858214\n",
      "2024-07-24 16:30:01,425 - INFO - ---- Fold 3 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[148]\ttraining's auc: 0.894343\tvalid_1's auc: 0.858214\n",
      "[LightGBM] [Info] Number of positive: 5641, number of negative: 40378\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1244\n",
      "[LightGBM] [Info] Number of data points in the train set: 46019, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122580 -> initscore=-1.968224\n",
      "[LightGBM] [Info] Start training from score -1.968224\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:30:03,254 - INFO - Validation AUC score for fold 3: 0.864428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[246]\ttraining's auc: 0.909123\tvalid_1's auc: 0.864428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:30:03,417 - INFO - ---- Fold 4 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5641, number of negative: 40378\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1238\n",
      "[LightGBM] [Info] Number of data points in the train set: 46019, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122580 -> initscore=-1.968224\n",
      "[LightGBM] [Info] Start training from score -1.968224\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:30:04,767 - INFO - Validation AUC score for fold 4: 0.862741\n",
      "2024-07-24 16:30:04,898 - INFO - ---- Fold 5 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[177]\ttraining's auc: 0.899212\tvalid_1's auc: 0.862741\n",
      "[LightGBM] [Info] Number of positive: 5642, number of negative: 40378\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1235\n",
      "[LightGBM] [Info] Number of data points in the train set: 46020, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122599 -> initscore=-1.968046\n",
      "[LightGBM] [Info] Start training from score -1.968046\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:30:06,111 - INFO - Validation AUC score for fold 5: 0.858663\n",
      "2024-07-24 16:30:06,241 - INFO - Overall ROC-AUC Score for LightGBM: 0.862394 ± 0.003640\n",
      "2024-07-24 16:30:06,244 - INFO - LightGBM models and predictions saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[145]\ttraining's auc: 0.894664\tvalid_1's auc: 0.858663\n"
     ]
    }
   ],
   "source": [
    "# Define LightGBM parameters\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'reg_alpha': 0.03432385172267505,\n",
    "    'reg_lambda': 0.2998279059616829,\n",
    "    'colsample_bytree': 0.790292183596673,\n",
    "    'subsample': 0.9046878168822107,\n",
    "    'learning_rate': 0.05035039561309864,\n",
    "    'max_depth': 10,  # Further reduced max depth\n",
    "    'num_leaves': 31,  # Standard number of leaves\n",
    "    'min_child_samples': 100,  # Increased min child samples\n",
    "    'min_child_weight': 1,  # Adjusted min child weight\n",
    "    'min_split_gain': 0.09978597066868167,\n",
    "    'max_bin': 255,\n",
    "    # 'device': 'gpu',\n",
    "    'early_stopping_rounds': 50,\n",
    "    'verbose': 1  # Enable verbose mode\n",
    "}\n",
    "\n",
    "# Initialize lists to store out-of-fold predictions and AUC scores\n",
    "lgb_preds = []\n",
    "lgb_aucs = []\n",
    "\n",
    "# Train LightGBM model with cross-validation\n",
    "logging.info(\"Starting LightGBM training...\")\n",
    "for fold, (train_idx, test_idx) in enumerate(skfold.split(X, y)):\n",
    "    logging.info(f\"---- Fold {fold + 1} ----\")\n",
    "    \n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_valid, y_valid = X.iloc[test_idx], y.iloc[test_idx]\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data,\n",
    "        num_boost_round=3000,\n",
    "        valid_sets=[train_data, valid_data],\n",
    "    )\n",
    "    \n",
    "    valid_preds = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "    auc_score = roc_auc_score(y_valid, valid_preds)\n",
    "    lgb_aucs.append(auc_score)\n",
    "    logging.info(f\"Validation AUC score for fold {fold + 1}: {auc_score:.6f}\")\n",
    "    \n",
    "    test_pred = model.predict(test_df, num_iteration=model.best_iteration)\n",
    "    lgb_preds.append(test_pred)\n",
    "    \n",
    "    # Save the model for this fold\n",
    "    joblib.dump(model, f'lgb_model_fold_{fold + 1}.pkl')\n",
    "    \n",
    "    # Clear memory\n",
    "    del X_train, y_train, X_valid, y_valid, train_data, valid_data, model\n",
    "    gc.collect()\n",
    "\n",
    "# Calculate overall AUC score for LightGBM\n",
    "auc_mean_lgb = np.mean(lgb_aucs)\n",
    "auc_std_lgb = np.std(lgb_aucs)\n",
    "logging.info(f\"Overall ROC-AUC Score for LightGBM: {auc_mean_lgb:.6f} ± {auc_std_lgb:.6f}\")\n",
    "\n",
    "# Average the predictions from each fold for LightGBM\n",
    "test_pred_lgb = np.mean(lgb_preds, axis=0)\n",
    "joblib.dump(test_pred_lgb, 'test_pred_lgb.pkl')\n",
    "logging.info(\"LightGBM models and predictions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:30:06,258 - INFO - Starting XGBoost training...\n",
      "2024-07-24 16:30:06,267 - INFO - ---- Fold 1 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.81600\tvalid-auc:0.79088\n",
      "[100]\ttrain-auc:0.97729\tvalid-auc:0.86356\n",
      "[128]\ttrain-auc:0.98132\tvalid-auc:0.86345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:30:19,289 - INFO - Validation AUC score for fold 1: 0.865023\n",
      "2024-07-24 16:30:19,555 - INFO - ---- Fold 2 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.82021\tvalid-auc:0.79236\n",
      "[100]\ttrain-auc:0.97922\tvalid-auc:0.85517\n",
      "[120]\ttrain-auc:0.98229\tvalid-auc:0.85483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:30:33,386 - INFO - Validation AUC score for fold 2: 0.856486\n",
      "2024-07-24 16:30:33,649 - INFO - ---- Fold 3 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.82565\tvalid-auc:0.79679\n",
      "[100]\ttrain-auc:0.97883\tvalid-auc:0.86076\n",
      "[126]\ttrain-auc:0.98195\tvalid-auc:0.86018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:30:48,449 - INFO - Validation AUC score for fold 3: 0.862684\n",
      "2024-07-24 16:30:48,716 - INFO - ---- Fold 4 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.83482\tvalid-auc:0.79364\n",
      "[100]\ttrain-auc:0.97890\tvalid-auc:0.85769\n",
      "[175]\ttrain-auc:0.98962\tvalid-auc:0.85504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:31:05,670 - INFO - Validation AUC score for fold 4: 0.858279\n",
      "2024-07-24 16:31:06,009 - INFO - ---- Fold 5 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.82656\tvalid-auc:0.79528\n",
      "[93]\ttrain-auc:0.97640\tvalid-auc:0.85659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:31:17,561 - INFO - Validation AUC score for fold 5: 0.857371\n",
      "2024-07-24 16:31:17,777 - INFO - Overall ROC-AUC Score for XGBoost: 0.859969 ± 0.003306\n",
      "2024-07-24 16:31:17,780 - INFO - XGBoost models and predictions saved.\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost parameters\n",
    "xgb_params = {\n",
    "    'eval_metric': 'auc',\n",
    "    'eta': 0.05,\n",
    "    'alpha': 0.2545607592482198,\n",
    "    'subsample': 0.8388163485383147,\n",
    "    'colsample_bytree': 0.2732499701466825,\n",
    "    'max_depth': 16,\n",
    "    'min_child_weight': 5,\n",
    "    'gamma': 0.0017688666476104672,\n",
    "    'max_bin': 262143,\n",
    "    # 'tree_method': 'gpu_hist',  # Ensure your environment supports GPU\n",
    "    # 'predictor': 'gpu_predictor',  # Ensure your environment supports GPU\n",
    "    'enable_categorical': True,\n",
    "    'verbose': 100\n",
    "}\n",
    "\n",
    "# Initialize lists to store out-of-fold predictions and AUC scores\n",
    "xgb_preds = []\n",
    "xgb_aucs = []\n",
    "\n",
    "# Train XGBoost model with cross-validation\n",
    "logging.info(\"Starting XGBoost training...\")\n",
    "for fold, (train_idx, test_idx) in enumerate(skfold.split(X, y)):\n",
    "    logging.info(f\"---- Fold {fold + 1} ----\")\n",
    "    \n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_valid, y_valid = X.iloc[test_idx], y.iloc[test_idx]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid, enable_categorical=True)\n",
    "    \n",
    "    model = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain,\n",
    "        num_boost_round=3000,\n",
    "        evals=[(dtrain, 'train'), (dvalid, 'valid')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    valid_preds = model.predict(dvalid, iteration_range=(0, model.best_iteration))\n",
    "    auc_score = roc_auc_score(y_valid, valid_preds)\n",
    "    xgb_aucs.append(auc_score)\n",
    "    logging.info(f\"Validation AUC score for fold {fold + 1}: {auc_score:.6f}\")\n",
    "    \n",
    "    dtest = xgb.DMatrix(test_df, enable_categorical=True)\n",
    "    test_pred = model.predict(dtest, iteration_range=(0, model.best_iteration))\n",
    "    xgb_preds.append(test_pred)\n",
    "    \n",
    "    # Save the model for this fold\n",
    "    model.save_model(f'xgb_model_fold_{fold + 1}.json')\n",
    "    \n",
    "    # Clear memory\n",
    "    del X_train, y_train, X_valid, y_valid, dtrain, dvalid, model\n",
    "    gc.collect()\n",
    "\n",
    "# Calculate overall AUC score for XGBoost\n",
    "auc_mean_xgb = np.mean(xgb_aucs)\n",
    "auc_std_xgb = np.std(xgb_aucs)\n",
    "logging.info(f\"Overall ROC-AUC Score for XGBoost: {auc_mean_xgb:.6f} ± {auc_std_xgb:.6f}\")\n",
    "\n",
    "# Average the predictions from each fold for XGBoost\n",
    "test_pred_xgb = np.mean(xgb_preds, axis=0)\n",
    "joblib.dump(test_pred_xgb, 'test_pred_xgb.pkl')\n",
    "logging.info(\"XGBoost models and predictions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights - CatBoost: 0.3347, LightGBM: 0.3331, XGBoost: 0.3322\n",
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Normalize AUC scores to sum to 1\n",
    "total_auc = auc_mean_cat + auc_mean_lgb + auc_mean_xgb\n",
    "\n",
    "weight_cat = auc_mean_cat / total_auc\n",
    "weight_lgb = auc_mean_lgb / total_auc\n",
    "weight_xgb = auc_mean_xgb / total_auc\n",
    "\n",
    "# Print weights for verification\n",
    "print(f\"Weights - CatBoost: {weight_cat:.4f}, LightGBM: {weight_lgb:.4f}, XGBoost: {weight_xgb:.4f}\")\n",
    "\n",
    "# Blending predictions with calculated weights\n",
    "blended_preds = (weight_cat * test_pred_cat + weight_lgb * test_pred_lgb + weight_xgb * test_pred_xgb)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df.index,\n",
    "    'Response': blended_preds\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Submission file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition: Auto Insurance Prediction\n",
    "This notebook is designed to preprocess data, engineer features, train machine learning models, and blend predictions for the Kaggle Auto Insurance Prediction competition.\n",
    "\n",
    "## 1. Import Libraries\n",
    "Importing necessary libraries for data processing, model training, and evaluation.\n",
    "\n",
    "## 2. Memory Optimization\n",
    "Functions to reduce memory usage by optimizing data types of the dataframe.\n",
    "\n",
    "## 3. Data Preprocessing\n",
    "Preprocessing the dataset by mapping categorical values to numerical values and handling missing values.\n",
    "\n",
    "## 4. Feature Engineering\n",
    "Creating new features to enhance the predictive power of the models.\n",
    "\n",
    "## 5. Model Training and Validation\n",
    "Training CatBoost, LightGBM, and XGBoost models using Stratified K-Fold cross-validation.\n",
    "\n",
    "## 6. Blending Predictions\n",
    "Blending predictions from the three models based on their AUC scores to create the final submission.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
