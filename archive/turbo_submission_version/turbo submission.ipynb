{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create a log filename with the notebook name and current datetime\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_filename = f'kaggle_submission_{current_time}.log'\n",
    "\n",
    "# Configure logging to save to a file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()  # This ensures logs are also output to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "def get_column_stats(df):\n",
    "    \"\"\"Get basic statistics for each column in the dataframe.\"\"\"\n",
    "    stats = {}\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            stats[col] = {\n",
    "                'min': df[col].min(),\n",
    "                'max': df[col].max(),\n",
    "                'mean': df[col].mean(),\n",
    "                'unique': df[col].nunique()\n",
    "            }\n",
    "        else:\n",
    "            stats[col] = {\n",
    "                'unique': df[col].nunique()\n",
    "            }\n",
    "    return stats\n",
    "\n",
    "def compare_stats(stats_before, stats_after):\n",
    "    \"\"\"Compare statistics before and after type conversion.\"\"\"\n",
    "    for col in stats_before:\n",
    "        if stats_before[col] != stats_after[col]:\n",
    "            logging.warning(f\"Column {col} has changed: {stats_before[col]} != {stats_after[col]}\")\n",
    "\n",
    "def calculate_precision_loss(stats_before, stats_after):\n",
    "    \"\"\"Calculate and log precision loss for numeric columns.\"\"\"\n",
    "    for col in stats_before:\n",
    "        if 'mean' in stats_before[col]:\n",
    "            mean_before = stats_before[col]['mean']\n",
    "            mean_after = stats_after[col]['mean']\n",
    "            precision_loss = abs(mean_before - mean_after) / abs(mean_before) * 100\n",
    "            logging.info(f\"Column {col} precision loss: {precision_loss:.6f}%\")\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        logging.info(f'Start memory usage of dataframe: {start_mem:.2f} MB')\n",
    "\n",
    "    stats_before = get_column_stats(df)\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    stats_after = get_column_stats(df)\n",
    "    compare_stats(stats_before, stats_after)\n",
    "    calculate_precision_loss(stats_before, stats_after)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        logging.info(f'End memory usage of dataframe: {end_mem:.2f} MB')\n",
    "        logging.info(f'Decreased by {(100 * (start_mem - end_mem) / start_mem):.1f}%')\n",
    "\n",
    "    return df\n",
    "\n",
    "def safe_map(df, column, mapping):\n",
    "    \"\"\"Map categorical values to numerical values and log any unknown categories.\"\"\"\n",
    "    unknown_categories = set(df[column]) - set(mapping.keys())\n",
    "    if unknown_categories:\n",
    "        logging.warning(f\"Unknown categories in column {column}: {unknown_categories}\")\n",
    "    df[column] = df[column].map(mapping)\n",
    "    return df\n",
    "\n",
    "def import_data(file, **kwargs):\n",
    "    \"\"\"Create a dataframe and optimize its memory usage.\"\"\"\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True, **kwargs)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess the dataset.\"\"\"\n",
    "    gender_mapping = {'Male': 1, 'Female': 0}\n",
    "    vehicle_damage_mapping = {'Yes': 1, 'No': 0}\n",
    "    vehicle_age_mapping = {'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2}\n",
    "    \n",
    "    df = safe_map(df, 'Gender', gender_mapping)\n",
    "    df = safe_map(df, 'Vehicle_Damage', vehicle_damage_mapping)\n",
    "    df = safe_map(df, 'Vehicle_Age', vehicle_age_mapping)\n",
    "    \n",
    "    df.drop(['Driving_License'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"Feature engineering on the dataset.\"\"\"\n",
    "    df['Previously_Insured_Annual_Premium'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Annual_Premium'].astype(str)))[0]\n",
    "    df['Previously_Insured_Vehicle_Age'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Vehicle_Age'].astype(str)))[0]\n",
    "    df['Previously_Insured_Vehicle_Damage'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Vehicle_Damage'].astype(str)))[0]\n",
    "    df['Previously_Insured_Vintage'] = pd.factorize((df['Previously_Insured'].astype(str) + df['Vintage'].astype(str)))[0]\n",
    "    return df\n",
    "\n",
    "def scale_dataset(df):\n",
    "    \"\"\"Scale the entire dataset using StandardScaler.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit and transform the scaler on the entire dataset\n",
    "    scaled_values = scaler.fit_transform(df)\n",
    "    \n",
    "    # Create a new DataFrame with the scaled values\n",
    "    scaled_df = pd.DataFrame(scaled_values, index=df.index, columns=df.columns)\n",
    "    \n",
    "    return scaled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths to datasets\n",
    "train_path = r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\train.csv\"\n",
    "test_path = r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\test.csv\"\n",
    "\n",
    "# Load and optimize data\n",
    "train_df = import_data(train_path, index_col='id')\n",
    "test_df = import_data(test_path, index_col='id')\n",
    "\n",
    "gc.collect()\n",
    "logging.info(\"Data loaded successfully.\")\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df = preprocess_data(train_df)\n",
    "test_df = preprocess_data(test_df)\n",
    "logging.info(\"Data preprocessed successfully.\")\n",
    "\n",
    "# Apply feature engineering\n",
    "train_df = feature_engineering(train_df)\n",
    "test_df = feature_engineering(test_df)\n",
    "\n",
    "gc.collect()\n",
    "logging.info(\"Feature engineering completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate features and target variable\n",
    "X = train_df.drop('Response', axis=1)\n",
    "y = train_df['Response']\n",
    "\n",
    "# Scale the entire dataset\n",
    "train_df = scale_dataset(train_df)\n",
    "test_df = scale_dataset(test_df)\n",
    "\n",
    "gc.collect()\n",
    "logging.info(f\"Features and target variable separated and scaled.\")\n",
    "\n",
    "# Define CatBoost parameters\n",
    "cat_params = {\n",
    "    'loss_function': 'Logloss',\n",
    "    'eval_metric': 'AUC',\n",
    "    'class_names': [0, 1],\n",
    "    'learning_rate': 0.075,\n",
    "    'iterations': 5500,\n",
    "    'depth': 9,\n",
    "    'random_strength': 0,\n",
    "    'l2_leaf_reg': 0.5,\n",
    "    'max_leaves': 512,\n",
    "    'fold_permutation_block': 64,\n",
    "    'random_seed': 42,\n",
    "    'allow_writing_files': False,\n",
    "    'verbose': 100  # Display log every 100 iterations\n",
    "}\n",
    "\n",
    "# Convert the test dataframe to strings and create the Pool object\n",
    "test_pool = Pool(test_df.astype(str), cat_features=X.columns.values)\n",
    "\n",
    "# Train CatBoost model on the entire training set\n",
    "train_pool = Pool(X.astype(str), y, cat_features=X.columns.values)\n",
    "model = CatBoostClassifier(**cat_params)\n",
    "model.fit(train_pool, verbose=500)\n",
    "\n",
    "# Predict on the test set using the Pool object\n",
    "test_preds = model.predict_proba(test_pool)[:, 1]\n",
    "\n",
    "# Create a submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df.index,\n",
    "    'Response': test_preds\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_filename = f'submission_{current_time}.csv'\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "logging.info(f\"Submission file {submission_filename} created successfully.\")\n",
    "print(f\"Submission file {submission_filename} created successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
